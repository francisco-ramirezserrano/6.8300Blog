<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simulating And Evolving 3D Virtual Creatures Generated by Foundation Models</title>
    <meta property="og:title"
          content="Simulating And Evolving 3D Virtual Creatures Generated by Foundation Models"/>

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;700&display=swap" rel="stylesheet">

    <link rel="shortcut icon" href="images/icon.ico">

    <style type="text/css">
        body {
            font-family: 'Inter', sans-serif;
            background-color: #e0f2fe; /* Tailwind sky-100 */
            color: #374151; /* Tailwind gray-700 */
        }
        /* Active ToC link */
        .toc-link.active {
            font-weight: 600;
            color: #0ea5e9; /* sky-500 */
            background-color: #f0f9ff; /* sky-50 */
            border-left: 3px solid #0ea5e9; /* sky-500 */
            padding-left: 0.5rem;
            margin-left: -0.75rem;
        }
        html {
            scroll-behavior: smooth;
        }
        /* Target specific elements for darker text if needed */
        h1, h2, h3, h4, h5, h6 {
            color: #1f2937; /* Tailwind gray-800 */
        }
        /* Styling for links within main content blocks */
        .main-content-block a {
            color: #059669; /* Tailwind emerald-600 */
            text-decoration: none;
        }
        .main-content-block a:hover {
            color: #047857; /* Tailwind emerald-700 */
            text-decoration: underline;
        }
        /* Reference styling */
        .reference-item > a:first-child {
            font-weight: bold;
            color: #4b5563; /* gray-600 */
            margin-right: 5px;
            text-decoration: none;
        }
        .reference-item {
            margin-bottom: 0.75rem;
            padding-left: 1.5rem;
            text-indent: -1.5rem; /* For hanging indent */
        }
        /* Iframe aspect ratio wrapper */
        .aspect-video {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            height: 0; /* Collapse the div to rely on padding-bottom */
            overflow: hidden; /* Hide anything outside the aspect ratio */
        }
        .aspect-video > iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none; /* Remove default iframe border */
        }
        .aspect-fish {
            position: relative;
            padding-bottom: 40%;   /* tweak this number to taste */
            height: 0;
            overflow: hidden;
          }
        .aspect-fish > iframe {
          position: absolute;
          inset: 0;              /* shorthand for top/right/bottom/left:0 */
          width: 100%;
          height: 100%;
          border: none;
        }
    </style>
</head>

<body class="bg-sky-100">

<div class="container mx-auto flex flex-wrap lg:flex-nowrap justify-center py-8 px-4">

    <aside class="w-full lg:w-1/6 flex-shrink-0 order-2 lg:order-1 lg:pr-4 mb-8 lg:mb-0">
        <div id="toc-container" class="sticky top-5">
            <b class="text-lg font-semibold mb-3 block text-gray-700">Outline</b>
            <nav id="toc-nav" class="space-y-2">
                <a href="#abstract" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">Abstract</a>
                <a href="#introduction" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">1. Introduction</a>
                <a href="#creature_generation_pipeline" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">2. Creature Generation Pipeline</a>
                <a href="#pipeline" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">3. Pipeline Details</a>
                <a href="#computational_design" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">4. Computational Design & Simulation</a>
                <a href="#evolvability" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">5. Evolvability</a>
                <a href="#evolutionary_loop" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">6. Evolutionary Loop</a>
                <a href="#results" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">7. Results</a>
                <a href="#discussion" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">8. Discussion</a>
                <a href="#acknowledgements" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">Acknowledgements</a>
                <a href="#references" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">References</a>
            </nav>
        </div>
    </aside>

    <main class="w-full lg:w-4/6 flex-shrink-0 order-1 lg:order-2 mb-8 lg:mb-0">

        <section id="header" class="bg-white rounded-xl shadow-md p-6 mb-8">
            <h1 class="text-3xl font-bold mb-2 text-gray-800">
                Simulating And Evolving 3D Virtual Creatures<br>
                Generated by Foundation Models
            </h1>
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-gray-600 mb-3">
                <span><a href="#" class="text-lg text-gray-700 mb-1">Francisco Ramirez Serrano</a></span>
            </div>
            <p class="text-sm text-gray-500">First-year EECS PhD student, MIT</p>
            <p class="text-sm text-gray-500"><i>Final project for 6.8300</i></p>
            <p class="text-sm text-gray-500 mb-3">
                Contact: <a href="mailto:framser@mit.edu" class="text-sky-600 hover:underline">framser@mit.edu</a>
            </p>
            <div class="text-sm text-gray-500 space-x-4">
                <span>Data/Code: <a href="https://github.com/francisco-ramirezserrano/6.8300Blog" class="text-sky-600 hover:underline">https://github.com/francisco-ramirezserrano/6.8300Blog</a></span>
            </div>
        </section>

        <section id="abstract" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">Abstract</h2>

            <div class="flex flex-col lg:flex-row gap-6 items-start"> 
                <div class="flex-1 prose prose-sm sm:prose-base max-w-none"> 
                    <p>
                        Capturing and evolving the richness of life observed on earth in simulation is a long-standing goal of the
                        artificial life field (alife). Recent advances in generative AI have made it possible to create
                        3D meshes of virtual creatures from text prompts. This paper presents a pipeline that
                        leverages these generative models to create a diverse population of 3D creatures, which are
                        then simulated and evolved in a physics-based environment. The pipeline includes three layers of evolution;
                        a prompt-level genotype is mutated to generate a base population, images of creatures are bred together, and finally, 
                        a differentiable simulation environment allows for
                        the co-evolution of creature morphology and control. The results show that the
                        generated creatures exhibit a wide range of morphologies and behaviors. To the best of my knowledge,
                        the first demonstration to leverage generative models for the evolution of virtual creatures in 3D substrates.

                    </p>
                </div>

              <div class="w-full lg:w-1/2 rounded-lg shadow-md border overflow-hidden aspect-fish">
                <iframe src="fish_viewers/FatFish.html"
                        title="Interactive Fish Animation 1"
                        loading="lazy"></iframe>
              </div>

            </div>
        </section>

        <section id="introduction" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">1. Introduction</h2>
            <div class="flex flex-wrap lg:flex-nowrap gap-6">
                <div class="flex-grow prose prose-sm sm:prose-base max-w-none">
                    <p class="mb-4">Advancements in Artificial Intelligence and foundation models enable onâ€‘demand generation of 3D topologies. These 3D models have received great attention for applications linked to engineering design, but have not yet been leveraged for the study of artificial life. The underlying prior information informing the generative design are datasets of all life on earth and these models are therefore perfectly positioned to generate "Life as it could be". This paper aims to uncover how forms generated by foundation models can be simulated and evolved to learn about general principles.</p>
                    <p class="mb-4">Artificial Life (Alife) aims to explore through simulation "life as it could be". Open-ended evolution is    <a href="#ref_hughes2024openendednessessentialartificialsuperhuman">[1]</a> that took place on earth, a large body of work focuses on Euclidean spaces of dimension three as they model physical space. Foundation models armed with prior knowledge of our world are able to generate on demand 3D objects representing known and unknown lifeforms. This paper aims to explore how such these shapes can be brought to life and evolved.</p>
                    <p class="mb-4">Multiple obstacles have prevented researchers from reproducing the evolution of natural life in simulation. Encodings and complexification, limited compute for both creature expansion but also enough generations, advanced and rich enough environments, the right evolutive pressures and serendipitous curriculum learning <a href="#ref_Vaxenburg2025">[2]</a>.</p>
                    <p class="mb-4">Advances in artificial intelligence technologies are being leveraged to advance artificial life research in multiple facets (Search, evolution pressure etc.) <a href="#ref_kumar2024automatingsearchartificiallife">[3]</a>.</p>
                    <p class="mb-4">This paper focuses on leveraging Mesh generators, which take in a prompt or an image and output a 3D shape, for the evolution of virtual creatures in 3D substrates. Attention in Generative CAD etc. <a href="#ref_Wong_2024">[4]</a> but less application to Alife.</p>
                    <p class="mb-4">Desired traits of Foundation models for our application.</p>
                    <p class="mb-4">3D worlds approximating real physics allow researchers to intuitively explore how the virtual creature encodings supports known animal-like life forms and function. Additionally, evolved solutions in simulation can be more intuitively physically embodied.</p>
                    <p class="mb-4">Presentation of DiffAqua, and concrete experiments <a href="#ref_motionPlanner">[5]</a>, <a href="#ref_KSims">[6, 7]</a>.</p>
                    <p class="mb-4">Promptâ€‘Level Genotype â€“ Introduces naturalâ€‘language prompts as a compact, expressive, and humanâ€‘interpretable genome.</p>
                    <p class="mb-4">Prompt-level genotypes.</p>
                    <figure class="w-full my-8">
                        <img src="images/Figure2.png" alt="Figure 2: Overview of the creature generation pipeline." class="w-full h-auto object-contain mx-auto border border-gray-200 rounded shadow-sm">
                        <figcaption class="text-center text-sm mt-2 text-gray-700">
                            Figure 2: A schematic representation of the creature generation pipeline, detailing the progression from textual prompts through image synthesis, 3D reconstruction, and functional analysis for evolutionary selection.
                        </figcaption>
                    </figure>
                    <p class="mb-4">Freeform Evolution requires tremendous compute and real-world priors are a way to accelerate the search for functional solutions/narrow the search space <a href="#ref_li2024generatingfreeformendoskeletalrobots">[8]</a>.</p>
                    <p class="mb-4">Traits of image generators including character consistency, Attribute Precision, cleaner voxelization boundaries, editâ€‘inâ€‘place all facilitate these pipelines.</p>
                    <p>2Dâ†’3D Bridging now possible â€“ Demonstrates a practical, scalable route from textual ideas to physicsâ€‘ready meshes with no artist in the loop.</p>
                </div>
                <div class="w-full lg:w-1/3 flex-shrink-0">
                    <figure>
                        <img src="./images/Figure1.png"
                             class="rounded-lg border border-gray-200"
                             alt="Figure 1: Phase transition energies">
                        <figcaption class="mt-2 text-sm text-gray-600 text-center">
                            <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                            Additionally, there are living forms which we have not yet observed, such as exoplanet life. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                            Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                        </figcaption>
                    </figure>
                </div>
            </div>
        </section>

<section id="creature_generation_pipeline" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">2. Creature Generation Pipeline</h2>
            <p class="mb-4">On representations. Because the mutations occur in the high-level feature space (via the generative process) rather than direct bit flips, <b>most offspring are valid</b> â€“ a critical requirement for evolvability.</p>
            <p class="mb-4">Free-form text prompts as an encoding layer have the advantage of being directly <b>human interpretable</b>, <b>richly descriptive</b> and easily <b>mutable</b> <a href="#ref_guo2025evopromptconnectingllmsevolutionary">[9]</a>, <a href="#ref_Wong_2023">[10]</a>. Prompts have the ability to directly affect the shape of virtual creatures through multiple mechanisms, including global known lifeforms as targets, attribute modifiers as descriptors, etc.</p>
            <p class="mb-4">Modern text-to-image generation models have made great strides in 2024â€“2025. Key advancements include character consistency across images <a href="#ref_tewel2024trainingfreeconsistenttexttoimagegeneration">[11]</a>, fine attribute control <a href="#ref_brooks2023instructpix2pixlearningfollowimage">[12]</a>, editability, and high resolution <a href="#ref_podell2023sdxlimprovinglatentdiffusion">[13]</a>, all of which help maintain coherence and precision as evolved creatures are visualized.</p>
            <p class="mb-4">2025 has seen a burst of tools and models for image-to-3D reconstruction <a href="#ref_tochilkin2024triposrfast3dobject">[14]</a> and text-to-3D synthesis <a href="#ref_zhu2024hifahighfidelitytextto3dgeneration">[15]</a>. These developments enable a prompt-to-3D pipeline: a prompt produces an image (the creatureâ€™s appearance) and then AI lifts that into a 3D mesh for use in virtual worlds or physics simulations. Fast Single-Image Reconstruction: feed-forward pass under 0.5 seconds. Significantly better 3D quality from text prompts. Large focus on usable, animation-ready geometry as well as Clean and Watertight Mesh Generation.</p>
            <p class="mb-4">A grand challenge is to reverse-engineer function from that form. Affordance and Mobility Inference: A line of recent work in computer vision and robotics focuses on understanding the affordances of 3D objects. For example, in the context of scenes, methods now can identify articulating parts and how they move (doors that rotate, drawers that slide, etc.) <a href="#ref_53213">[16]</a>. In our example, for DiffAqua, we might want to detect and animate fish parts.</p>

            <figure class="w-full my-8">
                <img src="images/Figure3.png" alt="Figure 2: Overview of the creature generation pipeline." class="w-full h-auto object-contain mx-auto border border-gray-200 rounded shadow-sm">
                <figcaption class="text-center text-sm mt-2 text-gray-700">
                    Figure 2: A schematic representation of the creature generation pipeline, detailing the progression from textual prompts through image synthesis, 3D reconstruction, and functional analysis for evolutionary selection.
                </figcaption>
            </figure>
            <p class="mb-4">Automated rigging <a href="#ref_liu2025riganythingtemplatefreeautoregressiverigging">[17]</a>, <a href="#ref_RigNet">[18]</a>.</p>
            <p class="mb-4">Translating this to evolved creatures: if we generate a creature mesh, we could apply similar techniques to predict joint locations (e.g. identify limb segments and likely hinge points at knees or shoulders). The model might recognize â€œthis looks like a leg that could bend hereâ€ or â€œthese wings probably flap along this edge.â€ Such an automatic rigging or motion inference is still in early stages, but the components are emerging. At a simpler level, just classifying the creatureâ€™s morphology (does it have legs? wings? wheels?) can inform what type of movement to attempt. This is analogous to how a human seeing a fantastical creature might guess â€œit seems to have fins, so maybe it swims.â€ By leveraging large datasets of 3D objects with known functions (e.g. Shape2Motion dataset for articulated objects), one can train models to extend those predictions to new, AI-generated forms.</p>
            <p class="mb-4">Evolving Controllers for AI-Generated Bodies: physics-augmented diffusion model <a href="#ref_wang2023diffusebot">[19]</a>, <a href="#ref_li2025dsoaligning3dgenerators">[20]</a>, <a href="#ref_guo2024physicallycompatible3dobject">[21]</a>.</p>
            <figure>
                <img src="./images/Figure4.png"
                        class="rounded-lg border border-gray-200"
                        alt="Figure 1: Phase transition energies">
                <figcaption class="mt-2 text-sm text-gray-600 text-center">
                    <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                    Additionally, there are living forms which we have not yet observed, such as exoplanet life. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                    Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                </figcaption>
            </figure>            
    
            <p class="mb-4">Simulation-Based Fitness. In April 2025, a joint team from Google DeepMind and HHMI Janelia unveiled the first wholeâ€‘body physics simulation of the fruit fly Drosophila melanogasterâ€”an anatomically detailed virtual insect that can both walk and fly realistically.</p>
            <p>The remainder of this paper is organized as follows: Section 2 reviews related work in generative AI for 3D content creation, evolutionary computation with generative models, and physics-based simulation for functional design. Section 3 details the proposed methodology, including the prompt-level genotype, the genotype-to-phenotype pipeline, and the evolutionary framework. Section 4 describes the experimental setup using the DiffAqua environment and presents the results. Section 5 discusses the findings, strengths, limitations, and broader implications of the approach. Finally, Section 6 concludes the paper and outlines future research directions.</p>
        </section>
        <section id="pipeline" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">3. Pipeline Details</h2>
            <h3 class="text-xl font-medium mt-6 mb-3">2.1 Related Work</h3>
            <h4 class="text-lg font-medium mt-4 mb-2">2.1.1. Advances in Text-to-Image Synthesis</h4>
            <p class="mb-4">Text-to-image (T2I) synthesis has seen remarkable progress, largely driven by diffusion models such as Stable Diffusion <a href="#ref_rombach2022highresolutionimagesynthesislatent">[22]</a>, DALL-E <a href="#ref_ramesh2021zeroshottexttoimagegeneration">[23]</a>, and Imagen <a href="#ref_saharia2022photorealistictexttoimagediffusionmodels">[24]</a>. These models can generate high-quality, diverse images from textual prompts. A key area of development is Visual Concept Mining (VCM) <a href="#ref_li2025comprehensivesurveyvisualconcept">[25]</a>, which aims to enhance the controllability of T2I models by allowing them to learn specific visual concepts (e.g., artistic styles, object features) from reference images, complementing textual inputs. This improved controllability is crucial for generating specific and consistent visual features for virtual creatures.</p>
            <h4 class="text-lg font-medium mt-4 mb-2">2.1.2. From 2D Images to 3D Models</h4>
            <p class="mb-4">Bridging the gap from 2D images to 3D models is a critical step. Current research in diffusion models for 3D generation  can be broadly classified into three categories: 2D space diffusion leveraging pretrained 2D models, 2D space diffusion without pretrained models, and diffusion processes operating directly in 3D space <a href="#ref_placeholder1">[P1]</a>.</p>
            <p>Techniques like DreamFusion <a href="#ref_placeholder1">[P1]</a> and Score Jacobian Chaining <a href="#ref_placeholder1">[P1]</a> utilize pretrained 2D diffusion models to optimize 3D representations (often Neural Radiance Fields, NeRFs) by scoring rendered 2D views. Image-to-3D mesh generation tools like Trellis <a href="#ref_placeholder2">[P2]</a>, employed in this work, aim to produce explicit mesh representations directly. Trellis and similar models <a href="#ref_placeholder1">[P1]</a> represent a scalable route to 3D assets, though challenges such as the "Janus problem" (multi-view inconsistency) persist and are active areas of research <a href="#ref_placeholder1">[P1]</a>. Efficient alternatives like DreamGaussian also contribute to this rapidly evolving field <a href="#ref_placeholder1">[P1]</a>, <a href="#ref_hsinying2025consistentsubjectgenerationcontrastive">[26]</a>.</p>
            <aside class="mt-4 p-3 bg-gray-50 rounded-lg border text-sm text-gray-600">
                Note [P1], [P2]: Placeholder citations used where the original LaTeX source had numbers (e.g., 1, 2) without corresponding cite keys. These should be replaced with proper references by linking to the correct ID below.
            </aside>
        </section>

        <section id="computational_design" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">4. Simulation Environment for the Virtual Creatures</h2>
            <p class="mb-4">DiffAqua is <a href="#ref_Ma_2021">[27]</a> is the chosen platform for this work as it provides a differentiable pipeline for the co-design of soft underwater swimmer geometries and their controllers. Its differentiability allows for efficient gradient-based optimization. Other simulation engines were considered such as DiffTaichi <a href="#ref_DiffTaichi">[28]</a> and <a href="#ref_huang2025dittogymlearningcontrolsoft">[36]</a>.</p>
            <h3 class="text-xl font-medium mt-6 mb-3">3.2 Proposed Methodology (Example Subsection)</h3>
            <h4 class="text-lg font-medium mt-4 mb-2">3.2.3. Step 3: Mesh Adaptation for Simulation (DiffAqua)</h4>
            <p class="mb-4">This step is critical for translating the raw 3D mesh into a format suitable for the DiffAqua soft body physics simulator. This involves voxelization and the definition of actuation (muscles).</p>
            <p class="mb-2"><b>Voxelization:</b> The mesh generated by Trellis is converted into a voxel-based representation. Method: A uniform grid voxelization approach is typically employed, though adaptive methods could be explored. Standard libraries or custom scripts can perform this conversion. The resolution of the voxel grid is a key parameter, balancing simulation fidelity against computational cost. General literature on voxelization from images provides context for this process.</p>
            <p class="mb-2"><b>Muscle Placement / Actuator Definition:</b> Defining how the creature is actuated is crucial for assessing its function. Automation Challenge: The automation and intelligence of this "muscle placement" step are of paramount importance for a truly AI-driven design pipeline. If this requires significant manual intervention, it becomes a bottleneck. Current Approach (Hypothesized based on user input): An automated or heuristic-based method is used to place actuators on the voxelized creature. This could involve:</p>
            <ul class="list-disc list-inside mb-3 ml-4 space-y-1">
                <li>Identifying prominent appendages or regions of the body from the geometry.</li>
                <li>Using simple rules (e.g., placing actuators along the main axis for undulation, or within limb-like structures).</li>
                <li>Potentially deriving cues from the initial prompt (e.g., "powerful flippers" might suggest actuator placement in flipper-like regions).</li>
            </ul>
            <figure>
                <img src="./images/Figure5.png"
                        class="rounded-lg border border-gray-200"
                        alt="Figure 1: Phase transition energies">
                <figcaption class="mt-2 text-sm text-gray-600 text-center">
                    <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                    Additionally, there are living forms which we have not yet observed, such as exoplanet life. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                    Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                </figcaption>
            </figure>             
            <p class="mb-4">Context from Simulators: DiffAqua itself is designed for co-designing geometry and controllers. SoftZoo provides interfaces for specifying muscle placement. DiffuseBot also parameterizes actuator placement for its soft robots.</p>
            <div class="overflow-x-auto">
    <table class="min-w-full bg-white border border-gray-300">
        <thead>
            <tr class="bg-gray-100">
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Swimming mode</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Lateral-bend region<br>(% of body length you should give actuators)</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Segments to actuate<br>(if BL = 40 voxels)</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Suggested sin_period<br>(&lambda; in voxel units&dagger;)</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Real-world exemplar</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Simulation time</th>
            </tr>
        </thead>
        <tbody>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Anguilliform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">90 â€“ 100 % (essentially the whole trunk)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">1 â€“ 40</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">12 (&approx; 0.3 BL; keep default)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">American eel ğ˜ˆğ˜¯ğ˜¨ğ˜¶ğ˜ªğ˜­ğ˜­ğ˜¢ ğ˜³ğ˜°ğ˜´ğ˜µğ˜³ğ˜¢ğ˜µğ˜¢ <a href="https://onlinelibrary.wiley.com/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">Wiley Online Library</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Sub-carangiform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">50 â€“ 65 % (posterior half to two-thirds)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">17 â€“ 40 (easiest: 17 â€“ 40 = last 24)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">28 (&approx; 0.7 BL)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Rainbow trout ğ˜–ğ˜¯ğ˜¤ğ˜°ğ˜³ğ˜©ğ˜ºğ˜¯ğ˜¤ğ˜©ğ˜¶ğ˜´ ğ˜®ğ˜ºğ˜¬ğ˜ªğ˜´ğ˜´ <a href="https://journals.biologists.com/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">The Company of Biologists</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Carangiform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">30 â€“ 40 % (last third)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">27 â€“ 40 (&approx; last 14)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">40 (&approx; 1 BL)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Atlantic mackerel ğ˜šğ˜¤ğ˜°ğ˜®ğ˜£ğ˜¦ğ˜³ ğ˜´ğ˜¤ğ˜°ğ˜®ğ˜£ğ˜³ğ˜¶ğ˜´ <a href="https://www.sciencedirect.com/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">ScienceDirect</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Thunniform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">10 â€“ 15 % (stiff body; narrow peduncle only)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">36 â€“ 40 (&approx; last 4â€“6)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">60 (&approx; 1.5 BL)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Yellowfin tuna ğ˜›ğ˜©ğ˜¶ğ˜¯ğ˜¯ğ˜¶ğ˜´ ğ˜¢ğ˜­ğ˜£ğ˜¢ğ˜¤ğ˜¢ğ˜³ğ˜¦ğ˜´ <a href="https://www.surface.syr.edu/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">SURFACE</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Ostraciiform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">&le; 5 % (rigid body; tail heaves)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">39 â€“ 40 (&approx; last 1â€“2)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">&ge; 120 or set phase-gradient = 0 (pure oscillation)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Yellow boxfish ğ˜–ğ˜´ğ˜µğ˜³ğ˜¢ğ˜¤ğ˜ªğ˜°ğ˜¯ ğ˜¤ğ˜¶ğ˜£ğ˜ªğ˜¤ğ˜¶ğ˜´</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
        </tbody>
    </table>
</div>
<p class="mt-2 text-sm text-gray-600">&dagger; BL = Body Length</p>
            <p class="mb-4">Novelty Potential: A sophisticated method for automated muscle placement, perhaps using geometric deep learning on the mesh or further LLM-based interpretation of the prompt to infer functional regions, could be a significant research contribution in its own right. This directly addresses the challenge of deriving function from AI-generated form.</p>
            <p><b>Material Properties:</b> Material properties for the soft body simulation in DiffAqua <a href="#ref_lindsey1978form">[38]</a> (e.g., Young's modulus, density) are assigned. Initially, these might be uniform across the creature's body. More advanced implementations could allow for heterogeneous material properties, potentially also guided by the prompt or evolved. SoftZoo, for example, allows for specifying body stiffness.</p>
        
            <div class="w-full space-y-4 mb-8">

                <div class="w-full rounded-lg shadow-md border overflow-hidden aspect-fish">
                    <iframe src="fish_viewers/boxfish.html"  title="Interactive Fish Animation 1"
                            loading="lazy"
                            class="w-full h-full border-0"></iframe>
                </div>

                <div class="w-full rounded-lg shadow-md border overflow-hidden aspect-fish">
                    <iframe src="fish_viewers/bluefintuna.html"  title="Interactive Fish Animation 2"
                            loading="lazy"
                            class="w-full h-full border-0"></iframe>
                </div>

                <div class="w-full rounded-lg shadow-md border overflow-hidden aspect-fish">
                    <iframe src="fish_viewers/Anguila.html"  title="Interactive Fish Animation 3"
                            loading="lazy"
                            class="w-full h-full border-0"></iframe>
                </div>


    </div>
        </section>

        <section id="evolvability" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">5. Evolvability</h2>
            <p class="mb-4">Mitigate diversity collapse by evolving in different representations.</p>
            <h3 class="text-xl font-medium mt-6 mb-3">2.2 Related Work (Continued)</h3>
            <h4 class="text-lg font-medium mt-4 mb-2">2.2.1. Prompts and LLMs as Genotypes & Evolutionary Operators</h4>
            <p class="mb-4"><a href="#ref_hemberg2024evolvingcodelargelanguage">[29]</a>, <a href="#ref_nisioti2024textlifereciprocalrelationship">[30]</a>, <a href="#ref_cai2024exploringimprovementevolutionarycomputation">[31]</a>, <a href="#ref_fernando2023promptbreederselfreferentialselfimprovementprompt">[32]</a>, <a href="#ref_bradley2023qualitydiversityaifeedback">[33]</a>, <a href="#ref_lehman2022evolutionlargemodels">[34]</a>.</p>
            <p class="mb-4">The idea of using LLMs to guide or perform evolutionary operations is gaining traction. LLM GP formalizes an LLM-based evolutionary algorithm for evolving code, where the LLM handles initialization, mutation, and crossover by processing and generating prompts that represent programs <a href="#ref_placeholder1">[P1]</a>. Several systems showcase this paradigm: Evolution through Large Models (ELM) uses LLMs as intelligent mutation operators for robotic morphologies; EvoPrompting and PromptBreeder evolve the prompts themselves for in-context learning; EvoLLM treats the LLM as an end-to-end evolutionary algorithm; and Quality-Diversity through AI Feedback (QDAIF) uses LLMs to vary and evaluate texts and code for diversity and quality <a href="#ref_placeholder2">[P2]</a>. LLMs are also being explored for broader improvements to EC, such as assisting in evolutionary strategy selection, optimizing population design, and enhancing operator design.</p>
            <aside class="mt-4 p-3 bg-gray-50 rounded-lg border text-sm text-gray-600">
                Note [P1], [P2]: Placeholder citations used where the original LaTeX source had numbers (e.g., 1, 2) without corresponding cite keys. These should be replaced with proper references by linking to the correct ID below.
            </aside>
        </section>
        
        <section id="evolutionary_loop" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">6. Evolutionary Loop and Termination Criteria</h2>
            <p>(Content for this section was missing in the provided LaTeX source)</p>
        </section>

        <section id="results" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">7. Results</h2>
            <p class="mb-4">Open-endedness and creativity <a href="#ref_hughes2024openendednessessentialartificialsuperhuman">[1]</a>, <a href="#ref_kamb2024analytictheorycreativityconvolutional">[35]</a>.</p>
            
            <figure>
                <img src="./images/Figure6.PNG"
                        class="rounded-lg border border-gray-200"
                        alt="Figure 1: Phase transition energies">
                <figcaption class="mt-2 text-sm text-gray-600 text-center">
                    <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                    Additionally, there are living forms which we have not yet observed, such as exoplanet life. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                    Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                </figcaption>
            </figure>             
            <p>(Detailed results content to be added)</p>
        </section>

        <section id="discussion" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">8. Discussion</h2>
            <p class="mb-4">In this particular system avalanche size can be approximated by the size $s$ of the genotype that gave rise to it, see <a href="#introduction">Figure 1</a>. We shall measure the distribution of these sizes $P(s)$ in the Artificial Life system Avida, which implements a population of self-replicating computer programs written in a simple machine language-like instruction set of $\mathcal{D}=24$ instructions, with programs of varying sequence length. In the course of self-replication, these programs produce mutant off-spring because the `copy` instruction they use is flawed at a rate $R$ errors per instruction copied, and adapt to an environment in which the performance of <i>logical</i> computations on externally provided numbers is akin to the catalysis of chemical reactions <a href="#ref_OBA">[36]</a>. In this <i>artificial chemistry</i> therefore, successful computations accelerate the metabolism (i.e., the CPU) of those strings that carry the <i>gene</i> (code) necessary to perform the trick, and any program discovering a new trick is the seed of another avalanche.</p>
            <aside class="mt-4 p-3 bg-gray-50 rounded-lg border text-sm text-gray-600">
                <i>Note: LaTeX math $s$, $P(s)$, $\mathcal{D}=24$, $R$ might not render correctly without MathJax/MathML setup. Consider replacing with plain text or images if needed. The reference \Cref{size} was interpreted as a link to Figure 1 based on context. The reference \citep{OBA} was mapped to the OBA entry in the bibliography.</i>
            </aside>
        </section>

        <section id="acknowledgements" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">Acknowledgements</h2>
            <p>I am very greatful for the insightful conversations with Jiaming Liu, Akarsh Kumar, Ettore Randazzo, Marcello Tania, Karl Sims and Andrew Spielberg, as well as to the whole ALIFE community at MIT club.</p>
        </section>

        <section id="references" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">References</h2>
            <div class='citation text-sm'>
                <div class="reference-item"><a href="#ref_hughes2024openendednessessentialartificialsuperhuman" id="ref_hughes2024openendednessessentialartificialsuperhuman">[1]</a> Hughes, E., Dennis, M., Parker-Holder, J., Behbahani, F., Mavalankar, A., Shi, Y., Schaul, T., & Rocktaschel, T. (2024). Open-Endedness is Essential for Artificial Superhuman Intelligence. arXiv preprint arXiv:2406.04268. <a href="https://arxiv.org/abs/2406.04268" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Vaxenburg2025" id="ref_Vaxenburg2025">[2]</a> Vaxenburg, R., Siwanowicz, I., Merel, J., Robie, A. A., Morrow, C., Novati, G., Stefanidi, Z., Both, G.-J., Card, G. M., Reiser, M. B., Botvinick, M. M., Branson, K. M., Tassa, Y., & Turaga, S. C. (2025). Whole-body physics simulation of fruit fly locomotion. <i>Nature</i>. <a href="http://dx.doi.org/10.1038/s41586-025-09029-4" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_kumar2024automatingsearchartificiallife" id="ref_kumar2024automatingsearchartificiallife">[3]</a> Kumar, A., Lu, C., Kirsch, L., Tang, Y., Stanley, K. O., Isola, P., & Ha, D. (2024). Automating the Search for Artificial Life with Foundation Models. arXiv preprint arXiv:2412.17799. <a href="https://arxiv.org/abs/2412.17799" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Wong_2024" id="ref_Wong_2024">[4]</a> Wong, M., Rios, T., Menzel, S., & Ong, Y. S. (2024, June). Prompt Evolutionary Design Optimization with Generative Shape and Vision-Language models. In <i>2024 IEEE Congress on Evolutionary Computation (CEC)</i> (pp. 1â€“8). IEEE. <a href="http://dx.doi.org/10.1109/CEC60901.2024.10611898" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_motionPlanner" id="ref_motionPlanner">[5]</a> Satoi, D., Hagiwara, M., Uemoto, A., Nakadai, H., & Hoshino, J. (2016). Unified motion planner for fishes with various swimming styles. <i>ACM Transactions on Graphics (TOG)</i>, 35(4), 1-15. <a href="https://doi.org/10.1145/2897824.2925977" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_10.1145/192161.192167" id="ref_10.1145/192161.192167">[6]</a> Sims, K. (1994). Evolving virtual creatures. In <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i> (pp. 15-22). <a href="https://doi.org/10.1145/192161.192167" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_KSims" id="ref_KSims">[7]</a> Sims, K. (2023). Evolving Virtual Creatures. In <i>Seminal Graphics Papers: Pushing the Boundaries, Volume 2</i> (Art. 73). ACM. <a href="https://doi.org/10.1145/3596711.3596785" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_li2024generatingfreeformendoskeletalrobots" id="ref_li2024generatingfreeformendoskeletalrobots">[8]</a> Li, M., Kong, L., & Kriegman, S. (2024). Generating Freeform Endoskeletal Robots. arXiv preprint arXiv:2412.01036. <a href="https://arxiv.org/abs/2412.01036" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_guo2025evopromptconnectingllmsevolutionary" id="ref_guo2025evopromptconnectingllmsevolutionary">[9]</a> Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., & Yang, Y. (2025). EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers. arXiv preprint arXiv:2309.08532. <a href="https://arxiv.org/abs/2309.08532" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Wong_2023" id="ref_Wong_2023">[10]</a> Wong, M., Ong, Y.-S., Gupta, A., Bali, K. K., & Chen, C. (2023, June). Prompt Evolution for Generative AI: A Classifier-Guided Approach. In <i>2023 IEEE Conference on Artificial Intelligence (CAI)</i> (pp. 226â€“229). IEEE. <a href="http://dx.doi.org/10.1109/cai54212.2023.00105" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_tewel2024trainingfreeconsistenttexttoimagegeneration" id="ref_tewel2024trainingfreeconsistenttexttoimagegeneration">[11]</a> Tewel, Y., Kaduri, O., Gal, R., Kasten, Y., Wolf, L., Chechik, G., & Atzmon, Y. (2024). Training-Free Consistent Text-to-Image Generation. arXiv preprint arXiv:2402.03286. <a href="https://arxiv.org/abs/2402.03286" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_brooks2023instructpix2pixlearningfollowimage" id="ref_brooks2023instructpix2pixlearningfollowimage">[12]</a> Brooks, T., Holynski, A., & Efros, A. A. (2023). InstructPix2Pix: Learning to Follow Image Editing Instructions. arXiv preprint arXiv:2211.09800. <a href="https://arxiv.org/abs/2211.09800" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_podell2023sdxlimprovinglatentdiffusion" id="ref_podell2023sdxlimprovinglatentdiffusion">[13]</a> Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., MÃ¼ller, J., Penna, J., & Rombach, R. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv preprint arXiv:2307.01952. <a href="https://arxiv.org/abs/2307.01952" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_tochilkin2024triposrfast3dobject" id="ref_tochilkin2024triposrfast3dobject">[14]</a> Tochilkin, D., Pankratz, D., Liu, Z., Huang, Z., Letts, A., Li, Y., Liang, D., Laforte, C., Jampani, V., & Cao, Y.-P. (2024). TripoSR: Fast 3D Object Reconstruction from a Single Image. arXiv preprint arXiv:2403.02151. <a href="https://arxiv.org/abs/2403.02151" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_zhu2024hifahighfidelitytextto3dgeneration" id="ref_zhu2024hifahighfidelitytextto3dgeneration">[15]</a> Zhu, J., Zhuang, P., & Koyejo, S. (2024). HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance. arXiv preprint arXiv:2305.18766. <a href="https://arxiv.org/abs/2305.18766" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_53213" id="ref_53213">[16]</a> Delitzas, A., Takmaz, A., Tombari, F., Pollefeys, M., & Engelmann, F. (2024). SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes. In <i>CVPR</i>. <a href="https://alexdelitzas.github.io/assets/pdf/SceneFun3D.pdf" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_liu2025riganythingtemplatefreeautoregressiverigging" id="ref_liu2025riganythingtemplatefreeautoregressiverigging">[17]</a> Liu, I., Xu, Z., Yifan, W., Tan, H., Xu, Z., Wang, X., Su, H., & Shi, Z. (2025). RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets. arXiv preprint arXiv:2502.09615. <a href="https://arxiv.org/abs/2502.09615" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_RigNet" id="ref_RigNet">[18]</a> Xu, Z., Zhou, Y., Kalogerakis, E., Landreth, C., & Singh, K. (2020). RigNet: Neural Rigging for Articulated Characters. <i>ACM Transactions on Graphics (TOG)</i>, 39.</div>
                <div class="reference-item"><a href="#ref_wang2023diffusebot" id="ref_wang2023diffusebot">[19]</a> Wang, T.-H., Zheng, J., Ma, P., Du, Y., Kim, B., Spielberg, A. E., Tenenbaum, J. B., Gan, C., & Rus, D. (2023). DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models. In <i>Thirty-seventh Conference on Neural Information Processing Systems</i>. <a href="https://openreview.net/forum?id=1zo4iioUEs" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_li2025dsoaligning3dgenerators" id="ref_li2025dsoaligning3dgenerators">[20]</a> Li, R., Zheng, C., Rupprecht, C., & Vedaldi, A. (2025). DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness. arXiv preprint arXiv:2503.22677. <a href="https://arxiv.org/abs/2503.22677" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_guo2024physicallycompatible3dobject" id="ref_guo2024physicallycompatible3dobject">[21]</a> Guo, M., Wang, B., Ma, P., Zhang, T., Owens, C. E., Gan, C., Tenenbaum, J. B., He, K., & Matusik, W. (2024). Physically Compatible 3D Object Modeling from a Single Image. arXiv preprint arXiv:2405.20510. <a href="https://arxiv.org/abs/2405.20510" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_rombach2022highresolutionimagesynthesislatent" id="ref_rombach2022highresolutionimagesynthesislatent">[22]</a> Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. arXiv preprint arXiv:2112.10752. <a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_ramesh2021zeroshottexttoimagegeneration" id="ref_ramesh2021zeroshottexttoimagegeneration">[23]</a> Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. arXiv preprint arXiv:2102.12092. <a href="https://arxiv.org/abs/2102.12092" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_saharia2022photorealistictexttoimagediffusionmodels" id="ref_saharia2022photorealistictexttoimagediffusionmodels">[24]</a> Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., & Norouzi, M. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487. <a href="https://arxiv.org/abs/2205.11487" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_li2025comprehensivesurveyvisualconcept" id="ref_li2025comprehensivesurveyvisualconcept">[25]</a> Li, Z., Li, J., Xiong, L., Fu, Z., & Li, Z. (2025). A Comprehensive Survey on Visual Concept Mining in Text-to-image Diffusion Models. arXiv preprint arXiv:2503.13576. <a href="https://arxiv.org/abs/2503.13576" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_hsinying2025consistentsubjectgenerationcontrastive" id="ref_hsinying2025consistentsubjectgenerationcontrastive">[26]</a> Lee, H.-Y., Chan, K. C. K., & Yang, M.-H. (2025). Consistent Subject Generation via Contrastive Instantiated Concepts. arXiv preprint arXiv:2503.24387. <a href="https://arxiv.org/abs/2503.24387" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Ma_2021" id="ref_Ma_2021">[27]</a> Ma, P., Du, T., Zhang, J. Z., Wu, K., Spielberg, A., Katzschmann, R. K., & Matusik, W. (2021). DiffAqua: a differentiable computational design pipeline for soft underwater swimmers with shape interpolation. <i>ACM Transactions on Graphics (TOG)</i>, 40(4), 1-14. <a href="http://dx.doi.org/10.1145/3450626.3459832" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_DiffTaichi" id="ref_DiffTaichi">[28]</a> Hu, Y., Anderson, L., Li, T.-M., Sun, Q., Carr, N., Ragan-Kelley, J., & Durand, F. (2019). DiffTaichi: Differentiable Programming for Physical Simulation. arXiv preprint arXiv:1910.00935. <a href="http://arxiv.org/abs/1910.00935" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_hemberg2024evolvingcodelargelanguage" id="ref_hemberg2024evolvingcodelargelanguage">[29]</a> Hemberg, E., Moskal, S., & O'Reilly, U.-M. (2024). Evolving Code with A Large Language Model. arXiv preprint arXiv:2401.07102. <a href="https://arxiv.org/abs/2401.07102" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_nisioti2024textlifereciprocalrelationship" id="ref_nisioti2024textlifereciprocalrelationship">[30]</a> Nisioti, E., Glanois, C., Najarro, E., Dai, A., Meyerson, E., Pedersen, J. W., Teodorescu, L., Hayes, C. F., Sudhakaran, S., & Risi, S. (2024). From Text to Life: On the Reciprocal Relationship between Artificial Life and Large Language Models. arXiv preprint arXiv:2407.09502. <a href="https://arxiv.org/abs/2407.09502" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_cai2024exploringimprovementevolutionarycomputation" id="ref_cai2024exploringimprovementevolutionarycomputation">[31]</a> Cai, J., Xu, J., Li, J., Ymauchi, T., Iba, H., & Tei, K. (2024). Exploring the Improvement of Evolutionary Computation via Large Language Models. arXiv preprint arXiv:2405.02876. <a href="https://arxiv.org/abs/2405.02876" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_fernando2023promptbreederselfreferentialselfimprovementprompt" id="ref_fernando2023promptbreederselfreferentialselfimprovementprompt">[32]</a> Fernando, C., Banarse, D., Michalewski, H., Osindero, S., & RocktÃ¤schel, T. (2023). Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. arXiv preprint arXiv:2309.16797. <a href="https://arxiv.org/abs/2309.16797" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_bradley2023qualitydiversityaifeedback" id="ref_bradley2023qualitydiversityaifeedback">[33]</a> Bradley, H., Dai, A., Teufel, H., Zhang, J., Oostermeijer, K., Bellagente, M., Clune, J., Stanley, K., Schott, G., & Lehman, J. (2023). Quality-Diversity through AI Feedback. arXiv preprint arXiv:2310.13032. <a href="https://arxiv.org/abs/2310.13032" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_lehman2022evolutionlargemodels" id="ref_lehman2022evolutionlargemodels">[34]</a> Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., & Stanley, K. O. (2022). Evolution through Large Models. arXiv preprint arXiv:2206.08896. <a href="https://arxiv.org/abs/2206.08896" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_kamb2024analytictheorycreativityconvolutional" id="ref_kamb2024analytictheorycreativityconvolutional">[35]</a> Kamb, M., & Ganguli, S. (2024). An analytic theory of creativity in convolutional diffusion models. arXiv preprint arXiv:2412.20292. <a href="https://arxiv.org/abs/2412.20292" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_huang2025dittogymlearningcontrolsoft" id="ref_huang2025dittogymlearningcontrolsoft">[36]</a> Huang, S., Chen, B., Xu, H., & Sitzmann, V. (2025). DittoGym: Learning to Control Soft Shape-Shifting Robots. arXiv preprint arXiv:2401.13231. <a href="https://arxiv.org/abs/2401.13231" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_OBA" id="ref_OBA">[37]</a> Ofria, C. and Brown, C.T. (1998). The Avida User's Manual. In Adami (1998). The Avida software is publicly available at ftp.krl.caltech.edu/pub/avida.</div>
                <div class="reference-item"><a href="#ref_lindsey1978form" id="ref_lindsey1978form">[38]</a> Lindsey, C.C. (1978). 1 - Form, Function, and Locomotory Habits in Fish. In W.S. Hoar & D.J. Randall (Eds.), <i>Locomotion</i> (Fish Physiology, Vol. 7, pp. 1-100). Academic Press. <a href="https://doi.org/10.1016/S1546-5098(08)60163-6" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_pun2025generating" id="ref_pun2025generating">[39]</a> Pun, A., Deng, K., Liu, R., Ramanan, D., Liu, C., & Zhu, J.-Y. (2025). Generating Physically Stable and Buildable LEGO Designs from Text. arXiv preprint arXiv:2505.05469. <a href="https://arxiv.org/abs/2505.05469" target="_blank" rel="noopener noreferrer">[Link]</a></div>

            </div>
        </section>
    </main>

    <div class="hidden lg:block lg:w-1/6 flex-shrink-0 order-3 lg:pl-4"></div>
</div>

<script>
document.addEventListener('DOMContentLoaded',()=>{
  const secs=[...document.querySelectorAll('section[id]')];
  const links=[...document.querySelectorAll('#toc-nav a.toc-link')];
  const spy=()=>{
    let cur='';
    secs.forEach(s=>{if(scrollY>=s.offsetTop-150)cur=s.id});
    links.forEach(l=>l.classList.toggle('active',l.getAttribute('href')==='#'+cur));
  };
  window.addEventListener('scroll',spy);spy();
});
</script>

</body>
</ht