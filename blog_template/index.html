<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simulating And Evolving 3D Virtual Creatures Generated by Foundation Models</title>
    <meta property="og:title"
          content="Simulating And Evolving 3D Virtual Creatures Generated by Foundation Models"/>

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;700&display=swap" rel="stylesheet">

    <link rel="shortcut icon" href="images/icon.ico">

    <style type="text/css">
        body {
            font-family: 'Inter', sans-serif;
            background-color: #e0f2fe; /* Tailwind sky-100 */
            color: #374151; /* Tailwind gray-700 */
        }
        /* Active ToC link */
        .toc-link.active {
            font-weight: 600;
            color: #0ea5e9; /* sky-500 */
            background-color: #f0f9ff; /* sky-50 */
            border-left: 3px solid #0ea5e9; /* sky-500 */
            padding-left: 0.5rem;
            margin-left: -0.75rem;
        }
        html {
            scroll-behavior: smooth;
        }
        /* Target specific elements for darker text if needed */
        h1, h2, h3, h4, h5, h6 {
            color: #1f2937; /* Tailwind gray-800 */
        }
        /* Styling for links within main content blocks */
        .main-content-block a {
            color: #059669; /* Tailwind emerald-600 */
            text-decoration: none;
        }
        .main-content-block a:hover {
            color: #047857; /* Tailwind emerald-700 */
            text-decoration: underline;
        }
        /* Reference styling */
        .reference-item > a:first-child {
            font-weight: bold;
            color: #4b5563; /* gray-600 */
            margin-right: 5px;
            text-decoration: none;
        }
        .reference-item {
            margin-bottom: 0.75rem;
            padding-left: 1.5rem;
            text-indent: -1.5rem; /* For hanging indent */
        }
        /* Iframe aspect ratio wrapper */
        .aspect-video {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            height: 0; /* Collapse the div to rely on padding-bottom */
            overflow: hidden; /* Hide anything outside the aspect ratio */
        }
        .aspect-video > iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none; /* Remove default iframe border */
        }
        .aspect-fish {
            position: relative;
            padding-bottom: 40%;   /* tweak this number to taste */
            height: 0;
            overflow: hidden;
        }
        .aspect-fish > iframe {
          position: absolute;
          inset: 0;             /* shorthand for top/right/bottom/left:0 */
          width: 100%;
          height: 100%;
          border: none;
        }
        /* Custom table styles for better readability and fitting */
        .comparison-table th, .comparison-table td {
            border: 1px solid #d1d5db; /* Tailwind gray-300 */
            padding: 0.5rem; /* p-2, reduced for space */
            text-align: left; /* Align text to left for readability */
            vertical-align: top; /* Align content to the top */
            font-size: 0.875rem; /* text-sm, for better fit */
        }
        .comparison-table thead th {
            background-color: #f3f4f6; /* Tailwind gray-100 */
            font-weight: 600; /* semibold */
            text-align: center; /* Center headers */
        }
        .comparison-table .method-label {
            font-weight: 600; /* semibold */
            background-color: #f9fafb; /* Tailwind gray-50 */
            min-width: 120px; /* Ensure method label column has some width */
        }
        .comparison-table img {
            max-width: 150px; /* Reduced max GIF width for better fit */
            height: auto;
            display: block; /* Allow margin auto to work */
            margin-left: auto;
            margin-right: auto;
            border-radius: 0.375rem; /* rounded-md */
            box-shadow: 0 2px 4px -1px rgba(0, 0, 0, 0.08), 0 1px 2px -1px rgba(0, 0, 0, 0.04); /* lighter shadow */
        }
        .comparison-table .visual-cell {
            width: 170px; /* Fixed width for visual column */
            text-align: center;
        }
         .comparison-table td ul {
            list-style-type: disc; /* Ensure bullets are visible */
            list-style-position: outside;
            padding-left: 1.25rem; /* Indent list items (increased slightly) */
            margin-top: 0.25rem;
        }
        .comparison-table td ul li {
            margin-bottom: 0.25rem;
        }
    </style>
</head>

<body class="bg-sky-100">

<div class="container mx-auto flex flex-wrap lg:flex-nowrap justify-center py-8 px-4">

    <aside class="w-full lg:w-1/6 flex-shrink-0 order-2 lg:order-1 lg:pr-4 mb-8 lg:mb-0">
        <div id="toc-container" class="sticky top-5">
            <b class="text-lg font-semibold mb-3 block text-gray-700">Outline</b>
            <nav id="toc-nav" class="space-y-2">
                <a href="#abstract" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">Summary</a>
                <a href="#introduction" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">1. Introduction</a>
                <a href="#creature_generation_pipeline" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">2. Creature Generation Pipeline</a>
                <a href="#pipeline" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">3. Pipeline Details</a>
                <a href="#computational_design" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">4. Computational Design & Simulation</a>
                <a href="#evolvability" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">5. Evolvability</a>
                <a href="#evolutionary_loop" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">6. Evolutionary Loop</a>
                <a href="#results" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">7. Results</a>
                <a href="#discussion" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">8. Discussion</a>
                <a href="#acknowledgements" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">Acknowledgements</a>
                <a href="#references" class="toc-link block text-sm text-gray-600 hover:text-sky-600 py-1 px-3 rounded-md">References</a>
            </nav>
        </div>
    </aside>

    <main class="w-full lg:w-4/6 flex-shrink-0 order-1 lg:order-2 mb-8 lg:mb-0">

        <section id="header" class="bg-white rounded-xl shadow-md p-6 mb-8">
            <h1 class="text-3xl font-bold mb-2 text-gray-800">
                Simulating And Evolving 3D Virtual Creatures<br>
                Generated by Foundation Models
            </h1>
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-gray-600 mb-3">
                <span><a href="#" class="text-lg text-gray-700 mb-1">Francisco Ramirez Serrano</a></span>
            </div>
            <p class="text-sm text-gray-500">First-year EECS PhD student, MIT</p>
            <p class="text-sm text-gray-500"><i>Final project for 6.8300</i></p>
            <p class="text-sm text-gray-500 mb-3">
                Contact: <a href="mailto:framser@mit.edu" class="text-sky-600 hover:underline">framser@mit.edu</a>
            </p>
            <div class="text-sm text-gray-500 space-x-4">
                <span>Data/Code: <a href="https://github.com/francisco-ramirezserrano/6.8300Blog" class="text-sky-600 hover:underline">https://github.com/francisco-ramirezserrano/6.8300Blog</a></span>
            </div>
        </section>

        <section id="abstract" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">Summary</h2>

            <div class="flex flex-col lg:flex-row gap-6 items-start"> 
                <div class="flex-1 prose prose-sm sm:prose-base max-w-none"> 
                    <p>
                        Capturing and evolving the richness of life observed on earth in simulation is a long-standing goal of the
                        artificial life field (alife). Recent advances in generative AI have made it possible to create
                        3D meshes of virtual creatures from text prompts. This paper presents a pipeline that
                        leverages these generative models to create a diverse population of 3D creatures, which are
                        then simulated and evolved in a physics-based environment. The pipeline includes three layers of shape evolution;
                        a prompt-level genotype is mutated to generate a base population, images of creatures are bred together, and finally, 
                        a differentiable simulation environment allows for
                        the co-evolution of creature morphology and control. The results show the advantages of our evolution paradigm accross three representations, as
                        generated creatures exhibit not only a wide range of morphologies but also functional behaviors. To the best of my knowledge,
                        the first demonstration to leverage generative models for the evolution of virtual creatures in 3D substrates.

                    </p>

                    <p class="mt-4 text-sm text-gray-600">
                        <span class="font-semibold">Class related topics:</span>
                        <span class="ml-2">#ProjectiveGeometry / Image Formation</span>
                        <span class="ml-2">#RepresentationLearning</span>
                        <span class="ml-2">#DiffusionModels</span>
                        <span class="ml-2">#VisionForEmbodiedAgents</span>
                        <span class="ml-2">#DifferentiableRendering</span>
                        <span class="ml-2">#NeuralSceneRepresentations</span>
                        <span class="ml-2">#Grid-based data structures</span>
                    </p>
                </div>

              <div class="w-full lg:w-1/2 rounded-lg shadow-md border overflow-hidden aspect-fish">
                <iframe src="fish_viewers/FatFish.html"
                        title="Interactive Fish Animation 1"
                        loading="lazy"></iframe>
              </div>

            </div>
        </section>

        <section id="introduction" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">1. Introduction</h2>
            <div class="flex flex-wrap lg:flex-nowrap gap-6">
                <div class="flex-grow prose prose-sm sm:prose-base max-w-none">
                    <p class="mb-4">Advancements in Artificial Intelligence and foundation models have enabled on‑demand generation of textured 3D topologies. These 3D models are receiving great attention for applications linked to engineering design <a href="#ref_pun2025generating">[39]</a> or animation <a href="#ref_zhao2025hunyuan3d20">[45]</a> <a href="#ref_yang2025hunyuan3d10">[46]</a>, but have not yet been leveraged for the study of artificial life and evolution.
                        The underlying priors informing the distributions from which generative models sample are datasets describing of all life on earth and these models are therefore perfectly positioned to generate "Life as it could be". This paper aims to uncover how forms generated by foundation models can be simulated and evolved to learn about general life principles.</p>
                    <p class="mb-4">One of our core motivations is to understand the open-ended novelty evolution seems to provide. Applying evolution-based principles may be key to unlock creativity/self-improvement for future Artificial Intelligence Agents/Architectures <a href="#ref_hughes2024openendednessessentialartificialsuperhuman">[1]</a> <a href="#ref_novikov2025alphaevolve">[40]</a>.</p>
                    <p class="mb-4">Multiple obstacles have prevented researchers from reproducing the evolution of natural life in simulation (Though animal digital twins are almost here; such as with the full simulation of a fruitfly from DeepMind <a href="#ref_Vaxenburg2025">[2]</a>). Encodings are typically handcrafted (CPPNs <a href="#ref_stanley2007cppn">[44] </a> Recursive Trees <a href="#ref_10.1145/192161.192167">[8]</a>) and do not undergo sufficient complexification (that is, to evolve detailed organs beyong coarse body shapes, such as eyes <a href="#ref_tiwary2024roadmap">[45]</a> <a href="#ref_tiwary2025eye">[46]</a>. Limited compute for creature expansion, insufficiently advanced and rich enough environments, and wrong evolutive pressures are preventing researchers from going much beyond Karl Sim's original 1994 work on Evolving Virtual Creatures <a href="#ref_KSims">[7]</a>.</p>
                    <p class="mb-4">Freeform Evolution <a href="#ref_li2024generatingfreeformendoskeletalrobots">[8]</a> requires tremendous compute and real-world priors are a way to accelerate the search for functional solutions/narrow the search space.</p>
                    <p class="mb-4">Advances in artificial intelligence technologies are being leveraged to advance artificial life research in multiple ways. Recently Akarsh et. al. <a href="#ref_kumar2024automatingsearchartificiallife">[3]</a> used foundation models for closed loop search of interesting emmergent behaviors for example. This work aims to be similarly substrate agnostic, as created 3D creatures could be immersed in any possible simulation.</p>
                    <p class="mb-4">This paper focuses on leveraging Mesh generators, which take in a prompt or an image and output a 3D shape, for the evolution of virtual creatures in 3D substrates.</p>

                    <figure class="w-full my-8">
                        <img src="images/Figure2.png" alt="Figure 2: Overview of the creature generation pipeline." class="w-full h-auto object-contain mx-auto border border-gray-200 rounded shadow-sm">
                        <figcaption class="text-center text-sm mt-2 text-gray-700">
                            Figure 2: A schematic representation of the creature generation pipeline, detailing the progression from textual prompts through image synthesis, 3D reconstruction, functionalty encoding and simulation.
                        </figcaption>
                    </figure>
                    <p class="mb-4">As a simulation substrate we build upon DiffAqua <a href="#ref_diffaqua">[5]</a>, a differenciable simulation engine for the co-evolution of morphology and control for water swimming soft bodies.</p>
                    <p class="mb-4">The remainder of this Blog is organized as follows: Section 2 gives an overview of the Prompt to 3D creature Pipeline, Section 3 explains the simulation process, Section 4 describes the experimental setup regarding evolution, Section 5 presents the reusults of the experiment. Finally, Section 7 concludes the paper and outlines future research directions.</p>

                </div>
                <div class="w-full lg:w-1/3 flex-shrink-0"> <figure>
                        <img src="./images/Figure1.png"
                             class="rounded-lg border border-gray-200"
                             alt="Figure 1: Phase transition energies">
                        <figcaption class="mt-2 text-sm text-gray-600 text-center">
                            <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                            Additionally, there are living forms which we have not yet observed, such as in exoplanets or at the bottom of the mariana's Trench. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                            Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                        </figcaption>
                    </figure>
                </div>
            </div> <h3 class="text-xl font-semibold mt-8 mb-4 pt-6 border-t border-gray-300 text-gray-800">Comparative Overview of Methods To Encode and Simulate Aquatic Creatures</h3>
            
            <div class="my-4 overflow-x-auto shadow-lg rounded-lg">
                <table class="min-w-full comparison-table">
                    <caption class="text-xl font-semibold my-4 text-gray-800 text-center sr-only">Comparison of 3D Shape and Function Generation Methods For Aquatic Creatures</caption> <thead>
                        <tr>
                            <th class="method-label">Method</th>
                            <th class="visual-cell">Visual</th>
                            <th>Genotype Encoding</th>
                            <th>Phenotype Support</th>
                            <th>Evolution Search</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="method-label">Freeform Evolution (2025)</td>
                            <td class="visual-cell">
                                <img src="gifs/freeform.gif" alt="Freeform GIF" onerror="this.onerror=null; this.src='https://placehold.co/150x112/E8E8E8/000000?text=Freeform.gif';">
                            </td>
                            <td>
                                Direct parameterization (e.g., CPPNs, L-systems) or graph-based structures. Can be highly abstract.
                                <br><a href="#ref_li2024generatingfreeformendoskeletalrobots" class="text-xs text-sky-600 hover:underline">[8] (example)</a>
                            </td>
                            <td>
                                Wide range, from abstract forms to physically simulated robots/creatures. Emphasis on novel morphologies.
                            </td>
                            <td>
                                Focus on exploring vast search spaces which leads to less recogizable forms.
                            </td>
                        </tr>
                        <tr>
                            <td class="method-label">Karl Sims' Creatures (1994)</td>
                            <td class="visual-cell">
                                <img src="gifs/KarlFish.gif" alt="Karl Fish GIF" onerror="this.onerror=null; this.src='https://placehold.co/150x112/E8E8E8/000000?text=KarlFish.gif';">
                            </td>
                            <td>
                                Directed graphs of genetic programming expressions defining morphology and neural controllers.
                                <br><a href="#ref_KSims" class="text-xs text-sky-600 hover:underline">[6, 7]</a>
                            </td>
                            <td>
                                3D block-based creatures with articulated joints and sensors, simulated in a physics environment.
                            </td>
                            <td>
                                Genetic Algorithms with operations like mutation and crossover on the graph-based genotypes.
                            </td>
                        </tr>
                        <tr>
                            <td class="method-label">This Work (2025)</td>
                            <td class="visual-cell">
                                <img src="gifs/ThisWork.gif" alt="This Work GIF" onerror="this.onerror=null; this.src='https://placehold.co/150x112/D0D0D0/000000?text=ThisWork.gif';">
                            </td>
                            <td>
                                Prompt-level storage as natural language, Image level storage as pixels, 3D forms encoded as gausseans.
                            </td>
                            <td>
                                Grids of soft voxels, and muscle voxels. Muscles contract/expand to a sine signal.
                            </td>
                            <td>
                                Multi-level: Prompt mutation/breeding, image-space evolution, differentiable simulation for morphology/control co-evolution.
                            </td>
                        </tr>
                        <tr>
                            <td class="method-label">DiffAqua (2021)</td>
                            <td class="visual-cell">
                                <img src="gifs/DiffAquaGif.gif" alt="DiffAqua GIF" onerror="this.onerror=null; this.src='https://placehold.co/150x112/E8E8E8/000000?text=DiffAqua.gif';">
                            </td>
                            <td>
                                Parameters defining voxelized soft body geometry and muscle actuation patterns (3D forms encoded as gausseans).
                                <br><a href="#ref_Ma_2021" class="text-xs text-sky-600 hover:underline">[27]</a>
                            </td>
                            <td>
                                Voxelized soft underwater swimmers.
                            </td>
                            <td>
                                Gradient-based optimization via differentiable physics simulation. Allows for efficient co-design of form and function.
                            </td>
                        </tr>
                        <tr>
                            <td class="method-label">Motion Planner Animations (2016)</td>
                            <td class="visual-cell">
                                <img src="gifs/MotionPlanner.gif" alt="Motion Planner GIF" onerror="this.onerror=null; this.src='https://placehold.co/150x112/E8E8E8/000000?text=MotionPlanner.gif';">
                            </td>
                            <td>
                                Focused on control parameters for pre-defined or existing fish morphologies.
                                <br><a href="#ref_motionPlanner" class="text-xs text-sky-600 hover:underline">[5]</a>
                            </td>
                            <td>
                                Supports various fish morphologies (12 swimming types) by adapting control strategies.
                            </td>
                            <td>
                                Optimization of motion controllers (e.g., for speed, maneuverability) for given morphologies. May involve search but less about de-novo body generation.
                            </td>
                        </tr>
                        <tr>
                            <td class="method-label">In-Game Worlds such as Minecraft (2018)</td>
                            <td class="visual-cell">
                                <img src="gifs/Salmon_BE.gif" alt="Minecraft/Game World Example GIF" onerror="this.onerror=null; this.src='https://placehold.co/150x112/E8E8E8/000000?text=Game+Worlds';">
                            </td>
                            <td>
                                <ul>
                                    <li>Game code defining entities & behaviors.</li>
                                    <li>Procedural generation algorithms (creature variations).</li>
                                    <li>Level data / configuration files.</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                    <li>Swimming fish (e.g., Minecraft: ~4 fish types + variants).</li>
                                    <li>Often voxel/block-based or stylized 3D models.</li>
                                    <li>Behaviors via AI scripts & game logic.</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                    <li>Primarily design-time evolution by developers.</li>
                                    <li>Procedural generation offers some variation (not open-ended ALife).</li>
                                </ul>
                            </td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

    <section id="creature_generation_pipeline" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">2. Creature Generation Pipeline</h2>
            <p class="mb-4">Text prompts as a base encoding layer have the advantage of being directly <b>human interpretable</b>, <b>richly descriptive</b> and easily <b>mutable</b> <a href="#ref_guo2025evopromptconnectingllmsevolutionary">[9]</a>, <a href="#ref_Wong_2023">[10]</a>. Prompts have the ability to directly affect the shape of virtual creatures through multiple mechanisms, including global known lifeforms as targets, attribute modifiers as descriptors, etc.</p>
            <p class="mb-4">Modern text-to-image generation models have made great strides in 2024–2025. Key advancements include character consistency across images <a href="#ref_tewel2024trainingfreeconsistenttexttoimagegeneration">[11]</a>, fine attribute control <a href="#ref_brooks2023instructpix2pixlearningfollowimage">[12]</a>, editability, and high resolution <a href="#ref_podell2023sdxlimprovinglatentdiffusion">[13]</a>, all of which help maintain coherence and precision as evolved creatures are visualized.</p>
            <p class="mb-4">Equal advances have been made for image-to-3D reconstruction <a href="#ref_tochilkin2024triposrfast3dobject">[14]</a> and text-to-3D synthesis <a href="#ref_zhu2024hifahighfidelitytextto3dgeneration">[15]</a>. These developments enable a prompt-to-3D pipeline: a prompt produces an image (the creature’s appearance) and then AI lifts that into a 3D mesh for use in virtual worlds or physics simulations. </p>
            <p class="mb-4">For the image synthesis stage of this project, we utilized <a href="https://github.com/comfyanonymous/ComfyUI" target="_blank" rel="noopener noreferrer" class="text-sky-600 hover:underline">ComfyUI</a>, a and modular graphical user interface that allows for the design and execution of image generation workflows through a node-based system. This flexibility was ideal for experimenting with and fine-tuning the generation process. Recently, the Highdream l1 Dev model was integrated into ComfyUI and was selected for this work due to its state-of-the-art image synthesis capabilities and as it was easily available localy.</p>
            <p class="mb-4">
            For converting the 2D synthesized images into 3D textured assets, we use Hunyuan3D 2.0, introduced by Zhao et al. (2025) <a href="#ref_zhao2025hunyuan3d20" class="text-sky-600 hover:underline">[42]</a>
            To implement this within our ComfyUI-based workflow, we follow the setup guide provided on the <a href="https://comfyui-wiki.com/en/tutorial/advanced/3d/huanyuan3d-2" target="_blank" rel="noopener noreferrer" class="text-sky-600 hover:underline">ComfyUI Wiki</a>.
            At the time of the project, pre-packaged 3D generation nodes within the platform did not support texture rendering.
            </p>
            <figure class="w-full my-8">
                <img src="images/Figure3.png" alt="Figure 2: Overview of the creature generation pipeline." class="w-full h-auto object-contain mx-auto border border-gray-200 rounded shadow-sm">
                <figcaption class="text-center text-sm mt-2 text-gray-700">
                    Figure 2: Screenshots from the creature generation pipeline, taken from ComfyUI. A) Passage from textual prompts through image synthesis. The prompt reads "A voxel style puffer fish on a white background" B) 3D reconstruction with texture generation from an image, in this case a Boxfish.
                </figcaption>
            </figure>

            <p class="mb-4">A grand challenge is to reverse-engineer function from that form. A line of recent work in computer vision and robotics focuses on understanding the affordances of 3D objects. For example, in the context of scenes, methods now can identify articulating parts and how they move (doors that rotate, drawers that slide, etc.) <a href="#ref_53213">[16]</a>. In our example, for DiffAqua, we might want to detect and animate fish parts (Fins, tail...).</p>
            <p class="mb-4">We could apply similar techniques to the mesh generation to predict joint locations (e.g. identify limb segments and likely hinge points at knees or shoulders). These techniques are used in Rignet <a href="ref_RigNet">[15]</a> and RigAnything <a href="ref_liu2025riganythingtemplatefreeautoregressiverigging">[15]</a>  The model might then recognize “this looks like a leg that could bend here” or “these wings probably flap along this edge.” At a simpler level, just classifying the creature’s morphology (does it have legs? wings? wheels?) can inform what type of movement to attempt. This is implemented in AnyTop <a href="=ref_gat2025anytop">[37]</a>.
                The approach of rigging and then animating is used in the FishMotion planner paper which implements 12 distinct fish types based on 4 base locomotion elements <a href="#ref_motionPlanner">[14]</a> 
                While I set-up and generated a skeleton using a RigNet, I found that subsequently classifying the body parts and animating them would take significant effort, be limited by the underlying skeleton locomotion datasets, and also possibly not be physically accurate.
                For this reason, I decided to use a physics based simulation engine. As DiffAqua represents muscles as Gausseans for the smooth interpolation to unlock gradient techniques, I attempted to use a purely Geometrical method involving a Hierarchical Gaussean Mixture Models (cite HGMM paper) / Medial Skeleton based technique to locate and position the actuators.
                The intuition is that geometry based techniques might allow for a more generalizable solution not based on priors.
             </p>
            <figure>
                <img src="./images/Figure4.png"
                         class="rounded-lg border border-gray-200"
                         alt="Figure 1: Phase transition energies">
                <figcaption class="mt-2 text-sm text-gray-600 text-center">
                    <b>Figure 3:</b> Attempts at infering function purely from form. A) An example generated creature that features fins, tentacles, and a tail. B) Geometrical based techniques to define actuators C) Animation based technique to define a rig, and skeleton weights to the mesh, to then remap a movement. 
                </figcaption>
            </figure>           
            <p class="mb-4">Evolving Controllers for AI-Generated Bodies with physics-augmented diffusion model is a promissing avenue to create meshes that are more well defined<a href="#ref_wang2023diffusebot">[19]</a>, <a href="#ref_li2025dsoaligning3dgenerators">[20]</a>, <a href="#ref_guo2024physicallycompatible3dobject">[21]</a>.</p>
        
        </section>
        <section id="pipeline" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">3. Pipeline Details</h2>
            <h3 class="text-xl font-medium mt-6 mb-3">2.1 Related Work</h3>
            <h4 class="text-lg font-medium mt-4 mb-2">2.1.1. Advances in Text-to-Image Synthesis</h4>
            <p class="mb-4">The image generation workflow within ComfyUI, using the <code>Highdream l1 Dev</code> model, involves several key components:
                <ul class="list-disc list-inside space-y-2 mb-3 ml-4">
                    <li><strong>Quadruple CLIP Loader:</strong> This node loads multiple (typically three for SD3-based models: two OpenCLIP variants and one CLIP ViT-L from OpenAI) text encoder models. These encoders, such as CLIP (Contrastive Language-Image Pre-training), are crucial for interpreting the textual prompts. By using multiple encoders, the system gains a more nuanced and comprehensive understanding of the desired image content and style, leading to improved prompt adherence.</li>
                    <li><strong>Diffusion Model Loader:</strong> This component is responsible for loading the core generative model weights – in this case, <code>Highdream l1 Dev</code>. This model contains the sophisticated U-Net architecture that performs the iterative denoising process central to diffusion-based image generation.</li>
                    <li><strong>VAE Loader:</strong> The Variational Autoencoder (VAE) is loaded via this node. The VAE is essential for compressing images into a lower-dimensional latent space (encoding) and for reconstructing images from this latent space back into pixel space (decoding). The diffusion process primarily operates in this computationally efficient latent space.</li>
                    <li><strong>Positive and Negative Prompts:</strong> These are text inputs provided by the user. The <em>positive prompt</em> describes the desired elements, attributes, and artistic style of the creature (e.g., "a bioluminescent jellyfish with intricate tendrils, vibrant colors, photorealistic"). The <em>negative prompt</em> specifies elements to avoid (e.g., "blurry, cartoonish, extra limbs, disfigured"). These prompts guide the diffusion model during generation.</li>
                    <li><strong>Model Sampling (SD3):</strong> This refers to the main generative step performed by the diffusion model, often compatible with Stable Diffusion 3 (SD3) architectures. The process starts with an initial latent representation (often random noise) and iteratively refines it over a series of steps, guided by the conditioned prompts, to produce a coherent latent image.</li>
                    <li><strong>KSampler (<code>KSampler</code>):</strong> This node executes the sampling loop. It takes the loaded diffusion model, the processed prompts (via CLIP), and an initial latent image. It then applies a chosen sampling algorithm (e.g., Euler, DPM++, Heun) for a defined number of steps to denoise the latent image. Key parameters here include the number of sampling steps (influencing detail vs. speed), and the CFG (Classifier-Free Guidance) scale, which controls how strongly the model adheres to the prompt.</li>
                    <li><strong>VAE Decode:</strong> Once the KSampler has produced the final refined latent representation, the VAE Decode node uses the decoder part of the loaded VAE to transform this latent image from the compact latent space back into the standard pixel space, yielding the final output image.</li>
                    <li><strong>EmptySD3LatentImage (Size 1024x1024, Batch Size):</strong> This node initializes an empty (or noise-filled) latent tensor that serves as the starting canvas for the diffusion process. The "SD3" in its name indicates compatibility with models like Stable Diffusion 3. The specified size, 1024x1024 pixels, determines the resolution of the final generated image once decoded by the VAE. The batch size parameter allows for generating multiple images in parallel from the same set of prompts and settings.</li>
                </ul>
This structured pipeline in ComfyUI provides granular control over each aspect of the image generation, enabling the creation of high-quality and specific visuals for the subsequent 3D reconstruction and simulation phases.
</p>
            <p class="mb-4">Text-to-image (T2I) synthesis has seen remarkable progress, largely driven by diffusion models such as Stable Diffusion <a href="#ref_rombach2022highresolutionimagesynthesislatent">[22]</a>, DALL-E <a href="#ref_ramesh2021zeroshottexttoimagegeneration">[23]</a>, and Imagen <a href="#ref_saharia2022photorealistictexttoimagediffusionmodels">[24]</a>. These models can generate high-quality, diverse images from textual prompts. A key area of development is Visual Concept Mining (VCM) <a href="#ref_li2025comprehensivesurveyvisualconcept">[25]</a>, which aims to enhance the controllability of T2I models by allowing them to learn specific visual concepts (e.g., artistic styles, object features) from reference images, complementing textual inputs. This improved controllability is crucial for generating specific and consistent visual features for virtual creatures.</p>
            <h4 class="text-lg font-medium mt-4 mb-2">2.1.2. From 2D Images to 3D Models</h4>
            <p class="mb-4">Bridging the gap from 2D images to 3D models is a critical step. Current research in diffusion models for 3D generation  can be broadly classified into three categories: 2D space diffusion leveraging pretrained 2D models, 2D space diffusion without pretrained models, and diffusion processes operating directly in 3D space <a href="#ref_placeholder1">[P1]</a>.</p>
            <p>Techniques like DreamFusion <a href="#ref_placeholder1">[P1]</a> and Score Jacobian Chaining <a href="#ref_placeholder1">[P1]</a> utilize pretrained 2D diffusion models to optimize 3D representations (often Neural Radiance Fields, NeRFs) by scoring rendered 2D views. Image-to-3D mesh generation tools like Trellis <a href="#ref_placeholder2">[P2]</a>, employed in this work, aim to produce explicit mesh representations directly. Trellis and similar models <a href="#ref_placeholder1">[P1]</a> represent a scalable route to 3D assets, though challenges such as the "Janus problem" (multi-view inconsistency) persist and are active areas of research <a href="#ref_placeholder1">[P1]</a>. Efficient alternatives like DreamGaussian also contribute to this rapidly evolving field <a href="#ref_placeholder1">[P1]</a>, <a href="#ref_hsinying2025consistentsubjectgenerationcontrastive">[26]</a>.</p>
        </section>

        <section id="computational_design" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">4. Simulation Environment for the Virtual Creatures</h2>
            <p class="mb-4">DiffAqua is <a href="#ref_Ma_2021">[27]</a> is the chosen platform for this work as it provides a differentiable computational design pipeline for the co-design of soft underwater swimmer geometries and their controllers. Its differentiability allows for efficient gradient-based optimization. Other simulation engines were considered such as DiffTaichi <a href="#ref_DiffTaichi">[28]</a> and <a href="#ref_huang2025dittogymlearningcontrolsoft">[36]</a>.</p>
            <h3 class="text-xl font-medium mt-6 mb-3">3.2 Proposed Methodology (Example Subsection)</h3>
            <h4 class="text-lg font-medium mt-4 mb-2">3.2.3. Step 3: Mesh Adaptation for Simulation (DiffAqua)</h4>
            <p class="mb-4">This step is critical for translating the raw 3D mesh into a format suitable for the DiffAqua soft body physics simulator. This involves voxelization and the definition of actuation (muscles).</p>
            <p class="mb-2"><b>Voxelization:</b> The mesh generated by Trellis is converted into a voxel-based representation. Method: A uniform grid voxelization approach is typically employed, though adaptive methods could be explored. Standard libraries or custom scripts can perform this conversion. The resolution of the voxel grid is a key parameter, balancing simulation fidelity against computational cost. General literature on voxelization from images provides context for this process.</p>
            <p class="mb-2"><b>Muscle Placement / Actuator Definition:</b> Defining how the creature is actuated is crucial for assessing its function. Automation Challenge: The automation and intelligence of this "muscle placement" step are of paramount importance for a truly AI-driven design pipeline. If this requires significant manual intervention, it becomes a bottleneck. Current Approach (Hypothesized based on user input): An automated or heuristic-based method is used to place actuators on the voxelized creature. This could involve:</p>
            <ul class="list-disc list-inside mb-3 ml-4 space-y-1">
                <li>Identifying prominent appendages or regions of the body from the geometry.</li>
                <li>Using simple rules (e.g., placing actuators along the main axis for undulation, or within limb-like structures).</li>
                <li>Potentially deriving cues from the initial prompt (e.g., "powerful flippers" might suggest actuator placement in flipper-like regions).</li>
            </ul>
            <figure>
                <img src="./images/Figure5.png"
                         class="rounded-lg border border-gray-200"
                         alt="Figure 1: Phase transition energies">
                <figcaption class="mt-2 text-sm text-gray-600 text-center">
                    <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                    Additionally, there are living forms which we have not yet observed, such as exoplanet life. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                    Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                </figcaption>
            </figure>           
            <p class="mb-4">Context from Simulators: DiffAqua itself is designed for co-designing geometry and controllers. SoftZoo provides interfaces for specifying muscle placement. DiffuseBot also parameterizes actuator placement for its soft robots.</p>
            <div class="overflow-x-auto">
    <table class="min-w-full bg-white border border-gray-300">
        <thead>
            <tr class="bg-gray-100">
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Swimming mode</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Lateral-bend region<br>(% of body length you should give actuators)</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Segments to actuate<br>(if BL = 40 voxels)</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Suggested sin_period<br>(&lambda; in voxel units&dagger;)</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Real-world exemplar</th>
                <th class="py-2 px-4 border-b text-left text-sm font-semibold text-gray-700">Simulation time</th>
            </tr>
        </thead>
        <tbody>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Anguilliform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">90 – 100 % (essentially the whole trunk)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">1 – 40</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">12 (&approx; 0.3 BL; keep default)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">American eel 𝘈𝘯𝘨𝘶𝘪𝘭𝘭𝘢 𝘳𝘰𝘴𝘵𝘳𝘢𝘵𝘢 <a href="https://onlinelibrary.wiley.com/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">Wiley Online Library</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Sub-carangiform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">50 – 65 % (posterior half to two-thirds)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">17 – 40 (easiest: 17 – 40 = last 24)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">28 (&approx; 0.7 BL)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Rainbow trout 𝘖𝘯𝘤𝘰𝘳𝘩𝘺𝘯𝘤𝘩𝘶𝘴 𝘮𝘺𝘬𝘪𝘴𝘴 <a href="https://journals.biologists.com/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">The Company of Biologists</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Carangiform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">30 – 40 % (last third)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">27 – 40 (&approx; last 14)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">40 (&approx; 1 BL)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Atlantic mackerel 𝘚𝘤𝘰𝘮𝘣𝘦𝘳 𝘴𝘤𝘰𝘮𝘣𝘳𝘶𝘴 <a href="https://www.sciencedirect.com/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">ScienceDirect</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Thunniform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">10 – 15 % (stiff body; narrow peduncle only)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">36 – 40 (&approx; last 4–6)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">60 (&approx; 1.5 BL)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Yellowfin tuna 𝘛𝘩𝘶𝘯𝘯𝘶𝘴 𝘢𝘭𝘣𝘢𝘤𝘢𝘳𝘦𝘴 <a href="https://www.surface.syr.edu/" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">SURFACE</a></td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
            <tr class="hover:bg-gray-50">
                <td class="py-2 px-4 border-b text-sm text-gray-700">Ostraciiform</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">&le; 5 % (rigid body; tail heaves)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">39 – 40 (&approx; last 1–2)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">&ge; 120 or set phase-gradient = 0 (pure oscillation)</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700">Yellow boxfish 𝘖𝘴𝘵𝘳𝘢𝘤𝘪𝘰𝘯 𝘤𝘶𝘣𝘪𝘤𝘶𝘴</td>
                <td class="py-2 px-4 border-b text-sm text-gray-700"></td>
            </tr>
        </tbody>
    </table>
</div>
<p class="mt-2 text-sm text-gray-600">&dagger; BL = Body Length</p>
            <p class="mb-4">Novelty Potential: A sophisticated method for automated muscle placement, perhaps using geometric deep learning on the mesh or further LLM-based interpretation of the prompt to infer functional regions, could be a significant research contribution in its own right. This directly addresses the challenge of deriving function from AI-generated form.</p>
            <p><b>Material Properties:</b> Material properties for the soft body simulation in DiffAqua <a href="#ref_lindsey1978form">[38]</a> (e.g., Young's modulus, density) are assigned. Initially, these might be uniform across the creature's body. More advanced implementations could allow for heterogeneous material properties, potentially also guided by the prompt or evolved. SoftZoo, for example, allows for specifying body stiffness.</p>
        
            <div class="w-full space-y-4 mb-8">

                <div class="w-full rounded-lg shadow-md border overflow-hidden aspect-fish">
                    <iframe src="fish_viewers/boxfish.html"  title="Interactive Fish Animation 1"
                            loading="lazy"
                            class="w-full h-full border-0"></iframe>
                </div>

                <div class="w-full rounded-lg shadow-md border overflow-hidden aspect-fish">
                    <iframe src="fish_viewers/bluefintuna.html"  title="Interactive Fish Animation 2"
                            loading="lazy"
                            class="w-full h-full border-0"></iframe>
                </div>

                <div class="w-full rounded-lg shadow-md border overflow-hidden aspect-fish">
                    <iframe src="fish_viewers/Anguila.html"  title="Interactive Fish Animation 3"
                            loading="lazy"
                            class="w-full h-full border-0"></iframe>
                </div>


    </div>
        </section>

        <section id="evolvability" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">5. Evolvability</h2>
            <p class="mb-4">Mitigate diversity collapse by evolving in different representations.</p>
            <h3 class="text-xl font-medium mt-6 mb-3">2.2 Related Work (Continued)</h3>
            <h4 class="text-lg font-medium mt-4 mb-2">2.2.1. Prompts and LLMs as Genotypes & Evolutionary Operators</h4>
            <p class="mb-4"><a href="#ref_hemberg2024evolvingcodelargelanguage">[29]</a>, <a href="#ref_nisioti2024textlifereciprocalrelationship">[30]</a>, <a href="#ref_cai2024exploringimprovementevolutionarycomputation">[31]</a>, <a href="#ref_fernando2023promptbreederselfreferentialselfimprovementprompt">[32]</a>, <a href="#ref_bradley2023qualitydiversityaifeedback">[33]</a>, <a href="#ref_lehman2022evolutionlargemodels">[34]</a>.</p>
            <p class="mb-4">The idea of using LLMs to guide or perform evolutionary operations is gaining traction. LLM GP formalizes an LLM-based evolutionary algorithm for evolving code, where the LLM handles initialization, mutation, and crossover by processing and generating prompts that represent programs <a href="#ref_placeholder1">[P1]</a>. Several systems showcase this paradigm: Evolution through Large Models (ELM) uses LLMs as intelligent mutation operators for robotic morphologies; EvoPrompting and PromptBreeder evolve the prompts themselves for in-context learning; EvoLLM treats the LLM as an end-to-end evolutionary algorithm; and Quality-Diversity through AI Feedback (QDAIF) uses LLMs to vary and evaluate texts and code for diversity and quality <a href="#ref_placeholder2">[P2]</a>. LLMs are also being explored for broader improvements to EC, such as assisting in evolutionary strategy selection, optimizing population design, and enhancing operator design.</p>
        </section>
        
        <section id="evolutionary_loop" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">6. Evolutionary Loop and Termination Criteria</h2>
            <p>(Content for this section was missing in the provided LaTeX source)</p>
        </section>

        <section id="results" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">7. Results</h2>
            <p class="mb-4">Open-endedness and creativity <a href="#ref_hughes2024openendednessessentialartificialsuperhuman">[1]</a>, <a href="#ref_kamb2024analytictheorycreativityconvolutional">[35]</a>.</p>
            
            <figure>
                <img src="./images/Figure6.PNG"
                         class="rounded-lg border border-gray-200"
                         alt="Figure 1: Phase transition energies">
                <figcaption class="mt-2 text-sm text-gray-600 text-center">
                    <b>Figure 1:</b> Generative AI leverages all data collected on observable living life, these are the known forms. 
                    Additionally, there are living forms which we have not yet observed, such as exoplanet life. Generative AI is particularly good at stitching data in uncommon ways, which leads to imaginary creatures which could conceivably move, these are known unknowns.
                    Finally, we hope that by leveraging freeform open-ended evolution we are able to leave learnt underlying distributions, and venture into unknown territory.</i>
                </figcaption>
            </figure>           
            <p>(Detailed results content to be added)</p>
        </section>

        <section id="discussion" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">8. Discussion</h2>
            <p class="mb-4">In this particular system avalanche size can be approximated by the size $s$ of the genotype that gave rise to it, see <a href="#introduction">Figure 1</a>. We shall measure the distribution of these sizes $P(s)$ in the Artificial Life system Avida, which implements a population of self-replicating computer programs written in a simple machine language-like instruction set of $\mathcal{D}=24$ instructions, with programs of varying sequence length. In the course of self-replication, these programs produce mutant off-spring because the `copy` instruction they use is flawed at a rate $R$ errors per instruction copied, and adapt to an environment in which the performance of <i>logical</i> computations on externally provided numbers is akin to the catalysis of chemical reactions <a href="#ref_OBA">[36]</a>. In this <i>artificial chemistry</i> therefore, successful computations accelerate the metabolism (i.e., the CPU) of those strings that carry the <i>gene</i> (code) necessary to perform the trick, and any program discovering a new trick is the seed of another avalanche.</p>
            <p class="mb-4">Focusing on 3D worlds approximating real physics allow researchers to intuitively explore if the chosen phenotype supports known animal-like life forms and function (cite Topoanything). Additionally, evolved solutions in simulation can be more intuitively physically embodied.</p>

            <aside class="mt-4 p-3 bg-gray-50 rounded-lg border text-sm text-gray-600">
                <i>Note: LaTeX math $s$, $P(s)$, $\mathcal{D}=24$, $R$ might not render correctly without MathJax/MathML setup. Consider replacing with plain text or images if needed. The reference \Cref{size} was interpreted as a link to Figure 1 based on context. The reference \citep{OBA} was mapped to the OBA entry in the bibliography.</i>
            </aside>
        </section>

        <section id="acknowledgements" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16 prose prose-sm sm:prose-base max-w-none">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">Acknowledgements</h2>
            <p>I am very greatful for the insightful conversations with Jiaming Liu, Akarsh Kumar, Ettore Randazzo, Marcello Tania, Karl Sims and Andrew Spielberg, as well as to the whole ALIFE community at MIT club.</p>
        </section>

        <section id="references" class="main-content-block bg-white rounded-xl shadow-md p-6 mb-10 scroll-mt-16">
            <h2 class="text-2xl font-semibold mb-4 border-b pb-2">References</h2>
            <div class='citation text-sm'>
                <div class="reference-item"><a href="#ref_hughes2024openendednessessentialartificialsuperhuman" id="ref_hughes2024openendednessessentialartificialsuperhuman">[1]</a> Hughes, E., Dennis, M., Parker-Holder, J., Behbahani, F., Mavalankar, A., Shi, Y., Schaul, T., & Rocktaschel, T. (2024). Open-Endedness is Essential for Artificial Superhuman Intelligence. arXiv preprint arXiv:2406.04268. <a href="https://arxiv.org/abs/2406.04268" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Vaxenburg2025" id="ref_Vaxenburg2025">[2]</a> Vaxenburg, R., Siwanowicz, I., Merel, J., Robie, A. A., Morrow, C., Novati, G., Stefanidi, Z., Both, G.-J., Card, G. M., Reiser, M. B., Botvinick, M. M., Branson, K. M., Tassa, Y., & Turaga, S. C. (2025). Whole-body physics simulation of fruit fly locomotion. <i>Nature</i>. <a href="http://dx.doi.org/10.1038/s41586-025-09029-4" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_kumar2024automatingsearchartificiallife" id="ref_kumar2024automatingsearchartificiallife">[3]</a> Kumar, A., Lu, C., Kirsch, L., Tang, Y., Stanley, K. O., Isola, P., & Ha, D. (2024). Automating the Search for Artificial Life with Foundation Models. arXiv preprint arXiv:2412.17799. <a href="https://arxiv.org/abs/2412.17799" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Wong_2024" id="ref_Wong_2024">[4]</a> Wong, M., Rios, T., Menzel, S., & Ong, Y. S. (2024, June). Prompt Evolutionary Design Optimization with Generative Shape and Vision-Language models. In <i>2024 IEEE Congress on Evolutionary Computation (CEC)</i> (pp. 1–8). IEEE. <a href="http://dx.doi.org/10.1109/CEC60901.2024.10611898" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_motionPlanner" id="ref_motionPlanner">[5]</a> Satoi, D., Hagiwara, M., Uemoto, A., Nakadai, H., & Hoshino, J. (2016). Unified motion planner for fishes with various swimming styles. <i>ACM Transactions on Graphics (TOG)</i>, 35(4), 1-15. <a href="https://doi.org/10.1145/2897824.2925977" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_10.1145/192161.192167" id="ref_10.1145/192161.192167">[6]</a> Sims, K. (1994). Evolving virtual creatures. In <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i> (pp. 15-22). <a href="https://doi.org/10.1145/192161.192167" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_KSims" id="ref_KSims">[7]</a> Sims, K. (2023). Evolving Virtual Creatures. In <i>Seminal Graphics Papers: Pushing the Boundaries, Volume 2</i> (Art. 73). ACM. <a href="https://doi.org/10.1145/3596711.3596785" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_li2024generatingfreeformendoskeletalrobots" id="ref_li2024generatingfreeformendoskeletalrobots">[8]</a> Li, M., Kong, L., & Kriegman, S. (2024). Generating Freeform Endoskeletal Robots. arXiv preprint arXiv:2412.01036. <a href="https://arxiv.org/abs/2412.01036" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_guo2025evopromptconnectingllmsevolutionary" id="ref_guo2025evopromptconnectingllmsevolutionary">[9]</a> Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., & Yang, Y. (2025). EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers. arXiv preprint arXiv:2309.08532. <a href="https://arxiv.org/abs/2309.08532" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Wong_2023" id="ref_Wong_2023">[10]</a> Wong, M., Ong, Y.-S., Gupta, A., Bali, K. K., & Chen, C. (2023, June). Prompt Evolution for Generative AI: A Classifier-Guided Approach. In <i>2023 IEEE Conference on Artificial Intelligence (CAI)</i> (pp. 226–229). IEEE. <a href="http://dx.doi.org/10.1109/cai54212.2023.00105" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_tewel2024trainingfreeconsistenttexttoimagegeneration" id="ref_tewel2024trainingfreeconsistenttexttoimagegeneration">[11]</a> Tewel, Y., Kaduri, O., Gal, R., Kasten, Y., Wolf, L., Chechik, G., & Atzmon, Y. (2024). Training-Free Consistent Text-to-Image Generation. arXiv preprint arXiv:2402.03286. <a href="https://arxiv.org/abs/2402.03286" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_brooks2023instructpix2pixlearningfollowimage" id="ref_brooks2023instructpix2pixlearningfollowimage">[12]</a> Brooks, T., Holynski, A., & Efros, A. A. (2023). InstructPix2Pix: Learning to Follow Image Editing Instructions. arXiv preprint arXiv:2211.09800. <a href="https://arxiv.org/abs/2211.09800" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_podell2023sdxlimprovinglatentdiffusion" id="ref_podell2023sdxlimprovinglatentdiffusion">[13]</a> Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., & Rombach, R. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv preprint arXiv:2307.01952. <a href="https://arxiv.org/abs/2307.01952" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_tochilkin2024triposrfast3dobject" id="ref_tochilkin2024triposrfast3dobject">[14]</a> Tochilkin, D., Pankratz, D., Liu, Z., Huang, Z., Letts, A., Li, Y., Liang, D., Laforte, C., Jampani, V., & Cao, Y.-P. (2024). TripoSR: Fast 3D Object Reconstruction from a Single Image. arXiv preprint arXiv:2403.02151. <a href="https://arxiv.org/abs/2403.02151" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_zhu2024hifahighfidelitytextto3dgeneration" id="ref_zhu2024hifahighfidelitytextto3dgeneration">[15]</a> Zhu, J., Zhuang, P., & Koyejo, S. (2024). HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance. arXiv preprint arXiv:2305.18766. <a href="https://arxiv.org/abs/2305.18766" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_53213" id="ref_53213">[16]</a> Delitzas, A., Takmaz, A., Tombari, F., Pollefeys, M., & Engelmann, F. (2024). SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes. In <i>CVPR</i>. <a href="https://alexdelitzas.github.io/assets/pdf/SceneFun3D.pdf" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_liu2025riganythingtemplatefreeautoregressiverigging" id="ref_liu2025riganythingtemplatefreeautoregressiverigging">[17]</a> Liu, I., Xu, Z., Yifan, W., Tan, H., Xu, Z., Wang, X., Su, H., & Shi, Z. (2025). RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets. arXiv preprint arXiv:2502.09615. <a href="https://arxiv.org/abs/2502.09615" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_RigNet" id="ref_RigNet">[18]</a> Xu, Z., Zhou, Y., Kalogerakis, E., Landreth, C., & Singh, K. (2020). RigNet: Neural Rigging for Articulated Characters. <i>ACM Transactions on Graphics (TOG)</i>, 39.</div>
                <div class="reference-item"><a href="#ref_wang2023diffusebot" id="ref_wang2023diffusebot">[19]</a> Wang, T.-H., Zheng, J., Ma, P., Du, Y., Kim, B., Spielberg, A. E., Tenenbaum, J. B., Gan, C., & Rus, D. (2023). DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models. In <i>Thirty-seventh Conference on Neural Information Processing Systems</i>. <a href="https://openreview.net/forum?id=1zo4iioUEs" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_li2025dsoaligning3dgenerators" id="ref_li2025dsoaligning3dgenerators">[20]</a> Li, R., Zheng, C., Rupprecht, C., & Vedaldi, A. (2025). DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness. arXiv preprint arXiv:2503.22677. <a href="https://arxiv.org/abs/2503.22677" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_guo2024physicallycompatible3dobject" id="ref_guo2024physicallycompatible3dobject">[21]</a> Guo, M., Wang, B., Ma, P., Zhang, T., Owens, C. E., Gan, C., Tenenbaum, J. B., He, K., & Matusik, W. (2024). Physically Compatible 3D Object Modeling from a Single Image. arXiv preprint arXiv:2405.20510. <a href="https://arxiv.org/abs/2405.20510" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_rombach2022highresolutionimagesynthesislatent" id="ref_rombach2022highresolutionimagesynthesislatent">[22]</a> Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. arXiv preprint arXiv:2112.10752. <a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_ramesh2021zeroshottexttoimagegeneration" id="ref_ramesh2021zeroshottexttoimagegeneration">[23]</a> Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. arXiv preprint arXiv:2102.12092. <a href="https://arxiv.org/abs/2102.12092" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_saharia2022photorealistictexttoimagediffusionmodels" id="ref_saharia2022photorealistictexttoimagediffusionmodels">[24]</a> Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., & Norouzi, M. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487. <a href="https://arxiv.org/abs/2205.11487" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_li2025comprehensivesurveyvisualconcept" id="ref_li2025comprehensivesurveyvisualconcept">[25]</a> Li, Z., Li, J., Xiong, L., Fu, Z., & Li, Z. (2025). A Comprehensive Survey on Visual Concept Mining in Text-to-image Diffusion Models. arXiv preprint arXiv:2503.13576. <a href="https://arxiv.org/abs/2503.13576" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_hsinying2025consistentsubjectgenerationcontrastive" id="ref_hsinying2025consistentsubjectgenerationcontrastive">[26]</a> Lee, H.-Y., Chan, K. C. K., & Yang, M.-H. (2025). Consistent Subject Generation via Contrastive Instantiated Concepts. arXiv preprint arXiv:2503.24387. <a href="https://arxiv.org/abs/2503.24387" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_Ma_2021" id="ref_Ma_2021">[27]</a> Ma, P., Du, T., Zhang, J. Z., Wu, K., Spielberg, A., Katzschmann, R. K., & Matusik, W. (2021). DiffAqua: a differentiable computational design pipeline for soft underwater swimmers with shape interpolation. <i>ACM Transactions on Graphics (TOG)</i>, 40(4), 1-14. <a href="http://dx.doi.org/10.1145/3450626.3459832" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_DiffTaichi" id="ref_DiffTaichi">[28]</a> Hu, Y., Anderson, L., Li, T.-M., Sun, Q., Carr, N., Ragan-Kelley, J., & Durand, F. (2019). DiffTaichi: Differentiable Programming for Physical Simulation. arXiv preprint arXiv:1910.00935. <a href="http://arxiv.org/abs/1910.00935" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_hemberg2024evolvingcodelargelanguage" id="ref_hemberg2024evolvingcodelargelanguage">[29]</a> Hemberg, E., Moskal, S., & O'Reilly, U.-M. (2024). Evolving Code with A Large Language Model. arXiv preprint arXiv:2401.07102. <a href="https://arxiv.org/abs/2401.07102" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_nisioti2024textlifereciprocalrelationship" id="ref_nisioti2024textlifereciprocalrelationship">[30]</a> Nisioti, E., Glanois, C., Najarro, E., Dai, A., Meyerson, E., Pedersen, J. W., Teodorescu, L., Hayes, C. F., Sudhakaran, S., & Risi, S. (2024). From Text to Life: On the Reciprocal Relationship between Artificial Life and Large Language Models. arXiv preprint arXiv:2407.09502. <a href="https://arxiv.org/abs/2407.09502" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_cai2024exploringimprovementevolutionarycomputation" id="ref_cai2024exploringimprovementevolutionarycomputation">[31]</a> Cai, J., Xu, J., Li, J., Ymauchi, T., Iba, H., & Tei, K. (2024). Exploring the Improvement of Evolutionary Computation via Large Language Models. arXiv preprint arXiv:2405.02876. <a href="https://arxiv.org/abs/2405.02876" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_fernando2023promptbreederselfreferentialselfimprovementprompt" id="ref_fernando2023promptbreederselfreferentialselfimprovementprompt">[32]</a> Fernando, C., Banarse, D., Michalewski, H., Osindero, S., & Rocktäschel, T. (2023). Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. arXiv preprint arXiv:2309.16797. <a href="https://arxiv.org/abs/2309.16797" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_bradley2023qualitydiversityaifeedback" id="ref_bradley2023qualitydiversityaifeedback">[33]</a> Bradley, H., Dai, A., Teufel, H., Zhang, J., Oostermeijer, K., Bellagente, M., Clune, J., Stanley, K., Schott, G., & Lehman, J. (2023). Quality-Diversity through AI Feedback. arXiv preprint arXiv:2310.13032. <a href="https://arxiv.org/abs/2310.13032" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_lehman2022evolutionlargemodels" id="ref_lehman2022evolutionlargemodels">[34]</a> Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., & Stanley, K. O. (2022). Evolution through Large Models. arXiv preprint arXiv:2206.08896. <a href="https://arxiv.org/abs/2206.08896" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_kamb2024analytictheorycreativityconvolutional" id="ref_kamb2024analytictheorycreativityconvolutional">[35]</a> Kamb, M., & Ganguli, S. (2024). An analytic theory of creativity in convolutional diffusion models. arXiv preprint arXiv:2412.20292. <a href="https://arxiv.org/abs/2412.20292" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_huang2025dittogymlearningcontrolsoft" id="ref_huang2025dittogymlearningcontrolsoft">[36]</a> Huang, S., Chen, B., Xu, H., & Sitzmann, V. (2025). DittoGym: Learning to Control Soft Shape-Shifting Robots. arXiv preprint arXiv:2401.13231. <a href="https://arxiv.org/abs/2401.13231" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_OBA" id="ref_OBA">[37]</a> Ofria, C. and Brown, C.T. (1998). The Avida User's Manual. In Adami (1998). The Avida software is publicly available at ftp.krl.caltech.edu/pub/avida.</div>
                <div class="reference-item"><a href="#ref_lindsey1978form" id="ref_lindsey1978form">[38]</a> Lindsey, C.C. (1978). 1 - Form, Function, and Locomotory Habits in Fish. In W.S. Hoar & D.J. Randall (Eds.), <i>Locomotion</i> (Fish Physiology, Vol. 7, pp. 1-100). Academic Press. <a href="https://doi.org/10.1016/S1546-5098(08)60163-6" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_pun2025generating" id="ref_pun2025generating">[39]</a> Pun, A., Deng, K., Liu, R., Ramanan, D., Liu, C., & Zhu, J.-Y. (2025). Generating Physically Stable and Buildable LEGO Designs from Text. arXiv preprint arXiv:2505.05469. <a href="https://arxiv.org/abs/2505.05469" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_novikov2025alphaevolve" id="ref_novikov2025alphaevolve">[40]</a> Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., & Balog, M. (2025). <span class="title">AlphaEvolve: A coding agent for scientific and algorithmic discovery</span>. Google DeepMind Technical Report. <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_gat2025anytop" id="ref_gat2025anytop">[41]</a> Gat, I., Raab, S., Tevet, G., Reshef, Y., Bermano, A. H., & Cohen-Or, D. (2025). <span class="title">AnyTop: Character Animation Diffusion with Any Topology</span>. arXiv preprint arXiv:2502.17327 [cs.GR]. <a href="https://arxiv.org/abs/2502.17327" target="_blank" rel="noopener noreferrer">[arXiv:2502.17327]</a></div>
                <div class="reference-item"><a href="#ref_zhao2025hunyuan3d20" id="ref_zhao2025hunyuan3d20">[42]</a> Zhao, Z., Lai, Z., Lin, Q., Zhao, Y., Liu, H., Yang, S., Feng, Y., Yang, M., Zhang, S., Yang, X., Shi, H., Liu, S., Wu, J., Lian, Y., Yang, F., Tang, R., He, Z., Wang, X., Liu, J., Zuo, X., Chen, Z., Lei, B., Weng, H., Xu, J., Zhu, Y., Liu, X., Xu, L., Hu, C., Yang, S., Zhang, S., Liu, Y., Huang, T., Wang, L., Zhang, J., Chen, M., Dong, L., Jia, Y., Cai, Y., Yu, J., Tang, Y., Zhang, H., Ye, Z., He, P., Wu, R., Zhang, C., Tan, Y., Xiao, J., Tao, Y., Zhu, J., Xue, J., Liu, K., Zhao, C., Wu, X., Hu, Z., Qin, L., Peng, J., Li, Z., Chen, M., Zhang, X., Niu, L., Wang, P., Kuang, H., Fan, Z., Zheng, X., Zhuang, W., He, Y., Liu, T., Yang, Y., Wang, D., Liu, Y., Jiang, J., Huang, J., & Guo, C. (2025). <span class="title">Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</span>. arXiv preprint arXiv:2501.12202 [cs.CV]. <a href="https://arxiv.org/abs/2501.12202" target="_blank" rel="noopener noreferrer">[arXiv:2501.12202]</a></div>
                <div class="reference-item"><a href="#ref_yang2025hunyuan3d10" id="ref_yang2025hunyuan3d10">[43]</a> Yang, X., Shi, H., Zhang, B., Yang, F., Wang, J., Zhao, H., Liu, X., Wang, X., Lin, Q., Yu, J., Wang, L., Xu, J., He, Z., Chen, Z., Liu, S., Wu, J., Lian, Y., Yang, S., Liu, Y., Yang, Y., Wang, D., Jiang, J., & Guo, C. (2025). <span class="title">Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation</span>. arXiv preprint arXiv:2411.02293 [cs.CV]. <a href="https://arxiv.org/abs/2411.02293" target="_blank" rel="noopener noreferrer">[arXiv:2411.02293]</a></div>
                <div class="reference-item"><a href="#ref_stanley2007cppn" id="ref_stanley2007cppn">[44]</a> Stanley, K. O. (2007). <span class="title">Compositional pattern producing networks: A novel abstraction of development</span>. <i>Genetic Programming and Evolvable Machines, 8</i>(2), 131-162. <a href="https://link.springer.com/article/10.1007/s10710-007-9028-8" target="_blank" rel="noopener noreferrer">[DOI]</a></div>
                <div class="reference-item"><a href="#ref_tiwary2024roadmap" id="ref_tiwary2024roadmap">[45]</a> Tiwary, K., Klinghoffer, T., Young, A., Somasundaram, S., Behari, N., Dave, A., Cheung, B., Nilsson, D.-E., Poggio, T., & Raskar, R. (2024). <span class="title">A Roadmap for Generative Design of Visual Intelligence</span>. <i>An MIT Exploration of Generative AI</i>. Published online September 18. <a href="https://mit-genai.pubpub.org/pub/bcfcb6lu" target="_blank" rel="noopener noreferrer">[Link]</a></div>
                <div class="reference-item"><a href="#ref_tiwary2025eye" id="ref_tiwary2025eye">[46]</a> Tiwary, K., Young, A., Tasneem, Z., Klinghoffer, T., Dave, A., Poggio, T., Nilsson, D.-E., Cheung, B., & Raskar, R. (2025). <span class="title">What if Eye...? Computationally Recreating Vision Evolution</span>. arXiv preprint arXiv:2501.15001 [cs.AI]. <a href="https://arxiv.org/abs/2501.15001" target="_blank" rel="noopener noreferrer">[arXiv:2501.15001]</a></div>
            </div>
        </section>
    </main>

    <div class="hidden lg:block lg:w-1/6 flex-shrink-0 order-3 lg:pl-4"></div>
</div>

<script>
document.addEventListener('DOMContentLoaded',()=>{
  const secs=[...document.querySelectorAll('section[id]')];
  const links=[...document.querySelectorAll('#toc-nav a.toc-link')];
  const spy=()=>{
    let cur='';
    secs.forEach(s=>{if(scrollY>=s.offsetTop-150)cur=s.id}); // Adjusted offset for better accuracy
    links.forEach(l=>l.classList.toggle('active',l.getAttribute('href')==='#'+cur));
  };
  window.addEventListener('scroll',spy);spy(); // Initial call to set active link on load
});
</script>

</body>
</html>
